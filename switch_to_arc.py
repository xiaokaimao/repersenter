#!/usr/bin/env python3
"""
ç®€å•çš„æ•°æ®é›†åˆ‡æ¢è„šæœ¬
æ¼”ç¤ºå¦‚ä½•åœ¨IMDBå’ŒARC-Challengeä¹‹é—´åˆ‡æ¢ï¼Œæ— éœ€é¢å¤–æ–‡ä»¶
"""

import sys
import os
sys.path.append('.')

def switch_dataset(target_dataset):
    """åˆ‡æ¢æ•°æ®é›†é…ç½®"""
    print(f"ğŸ”„ åˆ‡æ¢åˆ°{target_dataset.upper()}é…ç½®...")
    
    config_file = "config/settings.py"
    
    with open(config_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ•°æ®é›†æ˜ å°„
    dataset_configs = {
        'imdb': {'name': 'imdb', 'batch_size': 64},
        'arc': {'name': 'arc-challenge', 'batch_size': 32},
        'mmlu': {'name': 'mmlu', 'batch_size': 32}
    }
    
    if target_dataset not in dataset_configs:
        print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®é›†: {target_dataset}")
        return False
    
    config = dataset_configs[target_dataset]
    
    # æ›´æ–°æ•°æ®é›†å
    import re
    content = re.sub(
        r'TRANSFORMER_DATASET = "[^"]*"',
        f'TRANSFORMER_DATASET = "{config["name"]}"',
        content
    )
    
    # æ›´æ–°æ‰¹æ¬¡å¤§å°
    content = re.sub(
        r'TRANSFORMER_BATCH_SIZE = \d+',
        f'TRANSFORMER_BATCH_SIZE = {config["batch_size"]}',
        content
    )
    
    with open(config_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"âœ… å·²åˆ‡æ¢åˆ°{target_dataset.upper()}é…ç½®")
    print(f"  æ•°æ®é›†: {config['name']}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print("ç°åœ¨å¯ä»¥ç›´æ¥ä½¿ç”¨åŸæœ‰è„šæœ¬:")
    print("  python experiments/transformer/finetune.py")
    print("  python experiments/transformer/run_shapley.py")
    return True

def switch_to_arc():
    """åˆ‡æ¢åˆ°ARC-Challengeé…ç½®"""
    return switch_dataset('arc')

def switch_to_mmlu():
    """åˆ‡æ¢åˆ°MMLUé…ç½®"""
    return switch_dataset('mmlu')

def switch_to_imdb():
    """åˆ‡æ¢å›IMDBé…ç½®"""
    return switch_dataset('imdb')

def show_current_config():
    """æ˜¾ç¤ºå½“å‰é…ç½®"""
    import config.settings as settings
    
    config = settings.TRANSFORMER_FINETUNE_CONFIG
    
    print("ğŸ“‹ å½“å‰Transformeré…ç½®:")
    print(f"  æ•°æ®é›†: {config['dataset_name']}")
    print(f"  æ¨¡å‹: {config['model_name']}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"  å­¦ä¹ ç‡: {config['learning_rate']}")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")

def demonstrate_simplicity():
    """æ¼”ç¤ºç®€åŒ–åçš„ä½¿ç”¨æ–¹å¼"""
    print("ğŸ¯ ç®€åŒ–åçš„ä½¿ç”¨æ–¹å¼:")
    print("=" * 50)
    
    print("\n1ï¸âƒ£ è®­ç»ƒIMDBæ¨¡å‹:")
    print("   # ç¡®ä¿ TRANSFORMER_DATASET = 'imdb'")
    print("   python experiments/transformer/finetune.py")
    
    print("\n2ï¸âƒ£ è®­ç»ƒARC-Challengeæ¨¡å‹:")
    print("   # ä¿®æ”¹ TRANSFORMER_DATASET = 'arc-challenge'")
    print("   python experiments/transformer/finetune.py")
    
    print("\n3ï¸âƒ£ è®­ç»ƒMMLUæ¨¡å‹:")
    print("   # ä¿®æ”¹ TRANSFORMER_DATASET = 'mmlu'")
    print("   python experiments/transformer/finetune.py")
    
    print("\n4ï¸âƒ£ è®¡ç®—Shapleyå€¼ (å¯¹ä»»ä½•æ•°æ®é›†):")
    print("   python experiments/transformer/run_shapley.py")
    
    print("\nğŸ”‘ å…³é”®ç†è§£:")
    print("   - åº•å±‚éƒ½æ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡")
    print("   - åªéœ€è¦æ”¹å˜æ•°æ®é›†åç§°")
    print("   - å¾®è°ƒé€»è¾‘å®Œå…¨ç›¸åŒ")
    print("   - Shapleyè®¡ç®—ä¹Ÿå®Œå…¨ç›¸åŒ")
    
    print("\nğŸ’¡ æ•°æ®æ ¼å¼å·®å¼‚å¤„ç†:")
    print("   - IMDB: 'text' â†’ sentiment (0/1)")
    print("   - ARC: 'question + choices' â†’ answer (0/1/2/3)")
    print("   - MMLU: 'question + choices' â†’ answer (0/1/2/3)")
    print("   - æ•°æ®åŠ è½½å™¨è‡ªåŠ¨å¤„ç†æ ¼å¼è½¬æ¢")

def test_arc_with_existing_scripts():
    """æµ‹è¯•ç”¨ç°æœ‰è„šæœ¬å¤„ç†ARC"""
    print("\nğŸ§ª æµ‹è¯•ç°æœ‰è„šæœ¬å¤„ç†ARC...")
    
    try:
        # ä¸´æ—¶åˆ‡æ¢åˆ°ARC
        switch_to_arc()
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        import utils
        import importlib
        importlib.reload(utils.data_utils)  # é‡æ–°åŠ è½½é…ç½®
        
        print("æ­£åœ¨æµ‹è¯•ARCæ•°æ®åŠ è½½...")
        train_loader, test_loader, _, _, num_classes, original_labels, flipped_indices = utils.get_text_data_loaders(
            dataset_name="arc-challenge",
            tokenizer_name="/opt/models/Qwen3-0.6B-Base",
            batch_size=4,
            label_noise_rate=0.0
        )
        
        print(f"âœ… æµ‹è¯•æˆåŠŸ!")
        print(f"  æ•°æ®é›†: ARC-Challenge")
        print(f"  è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        print(f"  æµ‹è¯•æ ·æœ¬: {len(test_loader.dataset)}")
        print(f"  ç±»åˆ«æ•°: {num_classes}")
        
        # åˆ‡æ¢å›IMDB
        switch_to_imdb()
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        switch_to_imdb()  # ç¡®ä¿åˆ‡æ¢å›å»
        return False

if __name__ == "__main__":
    print("ğŸ”„ ç®€åŒ–çš„ARC-Challengeæ”¯æŒ")
    print("=" * 50)
    
    # æ˜¾ç¤ºå½“å‰é…ç½®
    show_current_config()
    
    # æ¼”ç¤ºç®€åŒ–æ–¹å¼
    demonstrate_simplicity()
    
    # æä¾›åˆ‡æ¢é€‰é¡¹
    print("\n" + "=" * 50)
    print("åˆ‡æ¢é€‰é¡¹:")
    print("1. åˆ‡æ¢åˆ°IMDB: python switch_to_arc.py imdb")
    print("2. åˆ‡æ¢åˆ°ARC-Challenge: python switch_to_arc.py arc") 
    print("3. åˆ‡æ¢åˆ°MMLU: python switch_to_arc.py mmlu")
    print("4. æµ‹è¯•å¤šæ•°æ®é›†æ”¯æŒ: python switch_to_arc.py test")
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        if command == "arc":
            switch_to_arc()
        elif command == "mmlu":
            switch_to_mmlu()
        elif command == "imdb":
            switch_to_imdb()
        elif command == "test":
            test_arc_with_existing_scripts()
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {command}")
            print("æ”¯æŒçš„å‘½ä»¤: imdb, arc, mmlu, test")
    
    print("\nğŸ‰ ç»“è®º: ä½ è¯´å¾—å¯¹!")
    print("å¤šé¡¹é€‰æ‹©é¢˜æ•°æ®é›† (ARC, MMLU) éƒ½å¯ä»¥ç”¨ç°æœ‰çš„transformerè„šæœ¬")
    print("åªéœ€è¦ä¿®æ”¹æ•°æ®é›†é…ç½®å³å¯ï¼Œä¸éœ€è¦é¢å¤–æ–‡ä»¶ï¼")
    print("éƒ½æ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„ä¸åŒè¡¨ç°å½¢å¼è€Œå·²ã€‚")