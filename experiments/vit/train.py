#!/usr/bin/env python3
"""
ViTè®­ç»ƒè„šæœ¬
ä½¿ç”¨é¢„è®­ç»ƒçš„Vision Transformerè¿›è¡Œå›¾åƒåˆ†ç±»å¾®è°ƒ

ç”¨æ³•:
    # å•GPUè®­ç»ƒ
    python experiments/vit/train.py
    
    # å¤šGPUè®­ç»ƒ
    accelerate launch experiments/vit/train.py
"""

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

import utils
import config.settings as settings

def main():
    # ä» settings.py åŠ è½½ViTé…ç½®
    config = settings.VIT_CONFIG
    dataset_name = config['dataset_name']
    batch_size = config['batch_size']
    device = torch.device(settings.DEVICE if torch.cuda.is_available() else "cpu")

    print(f"ğŸ¯ ViTå¾®è°ƒè®­ç»ƒ")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"æ¨¡å‹: {config['model_name']}")
    print(f"æ•°æ®é›†: {dataset_name}")
    print(f"å›¾åƒå¤§å°: {config['image_size']}")

    # --- è®¾ç½®éšæœºç§å­ ---
    utils.set_seed(settings.SEED)

    # --- æ•°æ®åŠ è½½ ---
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    train_loader, test_loader, _, _, num_classes, original_labels, flipped_indices = utils.get_vit_data_loaders(
        dataset_name=dataset_name,
        batch_size=batch_size,
        image_size=config['image_size'],
        label_noise_rate=config.get('label_noise_rate', 0.0)
    )
    
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_loader.dataset)}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_loader.dataset)}")
    print(f"ç±»åˆ«æ•°é‡: {num_classes}")
    if len(flipped_indices) > 0:
        print(f"æ ‡ç­¾å™ªå£°: {len(flipped_indices)} ä¸ªæ ·æœ¬ ({len(flipped_indices)/len(train_loader.dataset)*100:.1f}%)")

    # --- æ¨¡å‹æ„å»º ---
    print("ğŸ¤– æ„å»ºViTæ¨¡å‹...")
    model = utils.get_vit_model(
        model_name=config['model_name'],
        num_classes=num_classes,
        use_bf16=config.get('use_bf16', False),
        freeze_backbone=config.get('freeze_backbone', False)
    ).to(device)

    # --- å†»ç»“backboneï¼ˆå¦‚æœé…ç½®è¦æ±‚ï¼‰ ---
    if config.get('freeze_backbone', False):
        print("ğŸ”’ å†»ç»“ViT backboneï¼Œä»…è®­ç»ƒåˆ†ç±»å¤´...")
        trainable_params = []
        for name, param in model.named_parameters():
            if param.requires_grad:
                trainable_params.append(param)
                print(f"  - {name}")
    else:
        print("ğŸ”“ å¾®è°ƒæ•´ä¸ªViTæ¨¡å‹...")
        trainable_params = [p for p in model.parameters() if p.requires_grad]

    # --- å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ---
    criterion = nn.CrossEntropyLoss()
    
    optimizer = optim.AdamW(
        trainable_params,
        lr=config['learning_rate'],
        weight_decay=config['lambda_reg']
    )
    
    print(f"ä¼˜åŒ–å™¨: AdamW, å­¦ä¹ ç‡: {config['learning_rate']}, L2æ­£åˆ™: {config['lambda_reg']}")
    print(f"å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in trainable_params):,}")

    # --- è®­ç»ƒå¾ªç¯ ---
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_accuracy = 0.0
    output_dir = settings.CHECKPOINT_DIR
    os.makedirs(output_dir, exist_ok=True)
    model_save_path = os.path.join(output_dir, config['checkpoint_name'])

    for epoch in range(config["num_epochs"]):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['num_epochs']} [Train]")
        for batch_idx, (inputs, targets) in enumerate(train_bar):
            inputs, targets = inputs.to(device), targets.to(device)
            
            optimizer.zero_grad()
            
            # ViT forward pass
            if hasattr(model, 'forward') and 'pixel_values' in str(model.forward.__code__.co_varnames):
                outputs = model(pixel_values=inputs)
                logits = outputs.logits
            else:
                outputs = model(inputs)
                logits = outputs.logits if hasattr(outputs, 'logits') else outputs
            
            loss = criterion(logits, targets)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = logits.max(1)
            train_total += targets.size(0)
            train_correct += predicted.eq(targets).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            train_bar.set_postfix({
                'Loss': f'{train_loss/(batch_idx+1):.4f}',
                'Acc': f'{100.*train_correct/train_total:.2f}%'
            })

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            val_bar = tqdm(test_loader, desc=f"Epoch {epoch+1}/{config['num_epochs']} [Val]")
            for batch_idx, (inputs, targets) in enumerate(val_bar):
                inputs, targets = inputs.to(device), targets.to(device)
                
                # ViT forward pass
                if hasattr(model, 'forward') and 'pixel_values' in str(model.forward.__code__.co_varnames):
                    outputs = model(pixel_values=inputs)
                    logits = outputs.logits
                else:
                    outputs = model(inputs)
                    logits = outputs.logits if hasattr(outputs, 'logits') else outputs
                
                loss = criterion(logits, targets)
                val_loss += loss.item()
                _, predicted = logits.max(1)
                val_total += targets.size(0)
                val_correct += predicted.eq(targets).sum().item()
                
                # æ›´æ–°è¿›åº¦æ¡
                val_bar.set_postfix({
                    'Loss': f'{val_loss/(batch_idx+1):.4f}',
                    'Acc': f'{100.*val_correct/val_total:.2f}%'
                })

        # è®¡ç®—å‡†ç¡®ç‡
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        print(f"\nEpoch {epoch+1}/{config['num_epochs']}:")
        print(f"  è®­ç»ƒ - Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%")
        print(f"  éªŒè¯ - Loss: {val_loss/len(test_loader):.4f}, Acc: {val_acc:.2f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_accuracy:
            best_accuracy = val_acc
            torch.save(model.state_dict(), model_save_path)
            print(f"  ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_save_path} (å‡†ç¡®ç‡: {best_accuracy:.2f}%)")

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.2f}%")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {model_save_path}")
    
    # æ˜¾ç¤ºä¸‹ä¸€æ­¥æŒ‡ä»¤
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥:")
    print(f"1. è®¡ç®—Shapleyå€¼:")
    print(f"   python utils/shapley_utils.py --model-type vit")
    print(f"2. æˆ–è€…è¿è¡Œå¤šGPU Shapleyè®¡ç®—:")
    print(f"   accelerate launch utils/shapley_utils.py --model-type vit --accelerate")

if __name__ == "__main__":
    main()