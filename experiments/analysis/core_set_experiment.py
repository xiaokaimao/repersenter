# core_set_experiment.py - ç®€åŒ–ç‰ˆæœ¬

import torch
import numpy as np
import matplotlib.pyplot as plt
import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
import utils
import config.settings as settings
from utils.train_utils import train_model_with_accelerate

def run_core_set_experiment(experiment_type='resnet'):
    """
    ç®€åŒ–çš„æ ¸å¿ƒæ•°æ®é›†å®éªŒ - ä½¿ç”¨ç»Ÿä¸€çš„å¤šå¡è®­ç»ƒå‡½æ•°
    è¿è¡Œæ–¹å¼: accelerate launch experiments/analysis/core_set_experiment.py --type transformer
    """
    config = settings.CORE_SET_EXPERIMENT_CONFIG[experiment_type]
    
    # ä¸éœ€è¦å¤–éƒ¨acceleratorï¼Œè®©train_utilså†…éƒ¨å¤„ç†
    print(f"ğŸš€ Core-Setå®éªŒ ({experiment_type.upper()})")
    print(f"æ¨¡å‹: {config['model_name']}")
    print(f"æ•°æ®é›†: {config['dataset_name']}")
    print(f"Core-Setå¤§å°: {config['core_set_percent']}%")

    # --- 1. åŠ è½½Shapleyå€¼ ---
    if experiment_type == 'transformer':
        model_name_safe = config['model_name'].replace('/', '_')
        shapley_file_name = f"shapley_vectors_{config['dataset_name']}_{model_name_safe}_noise_{config.get('label_noise_rate', 0.0)}.pt"
    else:
        shapley_file_name = f"shapley_vectors_{config['dataset_name']}_{config['model_name']}_noise_{config.get('label_noise_rate', 0.0)}.pt"
    
    load_path = os.path.join(settings.RESULTS_DIR, shapley_file_name)
    
    if not os.path.exists(load_path):
        print(f"âŒ æ‰¾ä¸åˆ°Shapleyæ–‡ä»¶: {load_path}")
        return

    shapley_data = torch.load(load_path, map_location='cpu')
    shapley_vectors = shapley_data['shapley_vectors']
    usefulness_scores = torch.linalg.norm(shapley_vectors, dim=1).numpy()
    
    # --- 2. å‡†å¤‡æ•°æ® ---
    # ä½¿ç”¨ä¸åŸå§‹å¾®è°ƒç›¸åŒçš„batch size
    if experiment_type == 'transformer':
        batch_size = settings.TRANSFORMER_FINETUNE_CONFIG['batch_size']
        _, test_loader, train_set, _, num_classes, _, _ = utils.get_text_data_loaders(
            dataset_name=config['dataset_name'],
            tokenizer_name=config['model_name'],
            batch_size=batch_size,
            label_noise_rate=config.get('label_noise_rate', 0.0)
        )
        # è®¾ç½®collator
        from transformers import AutoTokenizer, DataCollatorWithPadding
        tokenizer = AutoTokenizer.from_pretrained(config['model_name'])
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)
    else:
        batch_size = settings.BATCH_SIZE
        _, test_loader, train_set, _, num_classes, _, _ = utils.get_data_loaders(
            dataset_name=config['dataset_name'], 
            batch_size=batch_size,
            shuffle_train=False,
            label_noise_rate=config.get('label_noise_rate', 0.0)
        )
        collate_fn = None
    
    # åˆ›å»ºæ ¸å¿ƒé›†
    n_train = len(train_set)
    core_set_size = int(n_train * config['core_set_percent'] / 100)
    
    sorted_indices = np.argsort(usefulness_scores)[::-1]
    shapley_core_indices = sorted_indices[:core_set_size]
    shapley_core_dataset = torch.utils.data.Subset(train_set, shapley_core_indices)
    
    np.random.seed(settings.SEED)
    random_core_indices = np.random.choice(n_train, core_set_size, replace=False)
    random_core_dataset = torch.utils.data.Subset(train_set, random_core_indices)
    
    # åˆ›å»ºDataLoader - ç®€åŒ–å¤„ç†ï¼Œè®©train_utilså¤„ç†accelerator prepare
    def make_loader(dataset, shuffle=True):
        return torch.utils.data.DataLoader(
            dataset, 
            batch_size=batch_size,
            shuffle=shuffle,
            collate_fn=collate_fn
        )
    
    full_train_loader = make_loader(train_set)
    shapley_core_loader = make_loader(shapley_core_dataset)
    random_core_loader = make_loader(random_core_dataset)
    
    print(f"å®Œæ•´è®­ç»ƒé›†: {n_train}")
    print(f"Shapleyæ ¸å¿ƒé›†: {len(shapley_core_dataset)}")
    print(f"éšæœºæ ¸å¿ƒé›†: {len(random_core_dataset)}")
    
    # --- 3. è®­ç»ƒé…ç½® ---
    # ä½¿ç”¨ä¸åŸå§‹å¾®è°ƒå®Œå…¨ç›¸åŒçš„é…ç½®
    if experiment_type == 'transformer':
        train_config = settings.TRANSFORMER_FINETUNE_CONFIG.copy()
        train_config['dataset_name'] = config['dataset_name']  # ç¡®ä¿æ•°æ®é›†åç§°æ­£ç¡®
    else:
        train_config = {
            'model_name': config['model_name'],
            'learning_rate': config['comparison_learning_rate'],
            'lambda_reg': config['lambda_reg'],
            'num_epochs': config['comparison_train_epochs'],
            'dataset_name': config['dataset_name']
        }
    
    # --- 4. è¿è¡Œä¸‰ä¸ªå®éªŒ ---
    print("\nğŸ”¥ å¼€å§‹ä¸‰ä¸ªè®­ç»ƒå®éªŒ...")
    
    experiments = [
        (full_train_loader, "å®Œæ•´æ•°æ®é›† (100%)"),
        (shapley_core_loader, f"Shapleyæ ¸å¿ƒé›† ({config['core_set_percent']}%)"),
        (random_core_loader, f"éšæœºæ ¸å¿ƒé›† ({config['core_set_percent']}%)")
    ]
    
    results = []
    
    for train_loader, exp_name in experiments:
        print(f"\n--- {exp_name} ---")
        
        # ç›´æ¥ä¼ é€’loaderï¼Œè®©train_utilså†…éƒ¨å¤„ç†accelerator prepare
        history = train_model_with_accelerate(
            train_loader=train_loader,
            test_loader=test_loader,
            model_config=train_config,
            num_classes=num_classes,
            experiment_name=exp_name,
            model_type=experiment_type,
            save_model=False  # æ ¸å¿ƒé›†å®éªŒä¸ä¿å­˜æ¨¡å‹
        )
        
        results.append(history['accuracy'][-1])
    
    # --- 5. å¯è§†åŒ–ç»“æœ ---
    final_acc_full, final_acc_shapley, final_acc_random = results
    
    exp_names = [
        f'å®Œæ•´æ•°æ®é›†\n(100%)',
        f'Shapleyæ ¸å¿ƒé›†\n({config["core_set_percent"]}%)',
        f'éšæœºæ ¸å¿ƒé›†\n({config["core_set_percent"]}%)'
    ]
    
    plt.figure(figsize=(10, 6))
    bars = plt.bar(exp_names, results, color=['#1f77b4', '#ff7f0e', '#2ca02c'])
    
    plt.ylabel('å‡†ç¡®ç‡ (%)')
    plt.title(f'Core-Setå®éªŒç»“æœ - {experiment_type.upper()}')
    plt.ylim(0, 100)
    
    for bar, acc in zip(bars, results):
        plt.text(bar.get_x() + bar.get_width()/2.0, acc + 1, 
                f'{acc:.2f}%', ha='center', va='bottom')
    
    plt.tight_layout()
    os.makedirs(settings.RESULTS_DIR, exist_ok=True)
    save_path = os.path.join(settings.RESULTS_DIR, f"core_set_simple_{experiment_type}.png")
    plt.savefig(save_path)
    print(f"\nğŸ“Š ç»“æœå›¾ä¿å­˜åˆ°: {save_path}")
    plt.close()
    
    print(f"\n=== ğŸ¯ å®éªŒç»“æœ ===")
    print(f"å®Œæ•´æ•°æ®é›†: {final_acc_full:.2f}%")
    print(f"Shapleyæ ¸å¿ƒé›†: {final_acc_shapley:.2f}%")
    print(f"éšæœºæ ¸å¿ƒé›†: {final_acc_random:.2f}%")
    print(f"Shapleyæå‡: {final_acc_shapley - final_acc_random:.2f}%")
    
    return {
        'full': final_acc_full,
        'shapley': final_acc_shapley, 
        'random': final_acc_random,
        'improvement': final_acc_shapley - final_acc_random
    }

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="ç®€åŒ–çš„æ ¸å¿ƒæ•°æ®é›†å®éªŒ")
    parser.add_argument("--type", choices=["resnet", "transformer"], 
                       default="resnet", help="å®éªŒç±»å‹")
    args = parser.parse_args()
    
    run_core_set_experiment(args.type)