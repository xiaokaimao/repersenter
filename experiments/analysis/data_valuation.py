"""
åŸºäºShapleyå€¼çš„æ•°æ®ä»·å€¼è¯„ä¼°
ç”¨äºè¯„ä¼°æ•°æ®æ ·æœ¬å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ï¼Œè¯†åˆ«é«˜ä»·å€¼å’Œä½ä»·å€¼æ•°æ®
"""
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
import config.settings as settings

def load_shapley_data(experiment_type='resnet'):
    """åŠ è½½Shapleyå€¼æ•°æ®"""
    config = settings.CORE_SET_EXPERIMENT_CONFIG[experiment_type]
    
    if experiment_type == 'transformer':
        model_name_safe = config['model_name'].replace('/', '_')
        shapley_file_name = f"shapley_vectors_{config['dataset_name']}_{model_name_safe}_noise_{config.get('label_noise_rate', 0.0)}.pt"
    else:
        shapley_file_name = f"shapley_vectors_{config['dataset_name']}_{config['model_name']}_noise_{config.get('label_noise_rate', 0.0)}.pt"
    
    load_path = os.path.join(settings.RESULTS_DIR, shapley_file_name)
    
    if not os.path.exists(load_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°Shapleyæ–‡ä»¶: {load_path}")
    
    return torch.load(load_path, map_location='cpu')

def calculate_value_metrics(shapley_vectors):
    """è®¡ç®—å¤šç§æ•°æ®ä»·å€¼æŒ‡æ ‡"""
    # L2èŒƒæ•° - æ•´ä½“ä»·å€¼
    l2_norms = torch.linalg.norm(shapley_vectors, dim=1).numpy()
    
    # L1èŒƒæ•° - ç¨€ç–æ€§åº¦é‡
    l1_norms = torch.norm(shapley_vectors, p=1, dim=1).numpy()
    
    # æœ€å¤§å€¼ - å¯¹æŸä¸ªç±»åˆ«çš„æœ€å¤§è´¡çŒ®
    max_values = torch.max(shapley_vectors, dim=1)[0].numpy()
    
    # æœ€å°å€¼ - å¯èƒ½çš„è´Ÿé¢å½±å“
    min_values = torch.min(shapley_vectors, dim=1)[0].numpy()
    
    # æ–¹å·® - è·¨ç±»åˆ«è´¡çŒ®çš„ä¸å‡åŒ€æ€§
    variances = torch.var(shapley_vectors, dim=1).numpy()
    
    # ç†µ - è´¡çŒ®çš„åˆ†æ•£ç¨‹åº¦
    # å…ˆè¿›è¡Œsoftmaxå½’ä¸€åŒ–ï¼Œç„¶åè®¡ç®—ç†µ
    softmax_shapley = torch.softmax(shapley_vectors, dim=1)
    entropies = -torch.sum(softmax_shapley * torch.log(softmax_shapley + 1e-8), dim=1).numpy()
    
    return {
        'l2_norm': l2_norms,
        'l1_norm': l1_norms,
        'max_value': max_values,
        'min_value': min_values,
        'variance': variances,
        'entropy': entropies
    }

def categorize_samples_by_value(value_metrics, num_categories=5):
    """æ ¹æ®ä»·å€¼æŒ‡æ ‡å¯¹æ ·æœ¬è¿›è¡Œåˆ†ç±»"""
    l2_norms = value_metrics['l2_norm']
    
    # ä½¿ç”¨åˆ†ä½æ•°è¿›è¡Œåˆ†ç±»
    percentiles = np.linspace(0, 100, num_categories + 1)
    thresholds = np.percentile(l2_norms, percentiles)
    
    categories = np.digitize(l2_norms, thresholds) - 1
    categories = np.clip(categories, 0, num_categories - 1)
    
    category_names = [
        'Very Low Value',
        'Low Value', 
        'Medium Value',
        'High Value',
        'Very High Value'
    ][:num_categories]
    
    return categories, category_names, thresholds

def cluster_samples_by_shapley(shapley_vectors, n_clusters=5):
    """åŸºäºShapleyå‘é‡å¯¹æ ·æœ¬è¿›è¡Œèšç±»"""
    # å¦‚æœç»´åº¦å¤ªé«˜ï¼Œå…ˆè¿›è¡ŒPCAé™ç»´
    if shapley_vectors.shape[1] > 50:
        pca = PCA(n_components=50)
        features = pca.fit_transform(shapley_vectors.numpy())
        print(f"PCAé™ç»´: {shapley_vectors.shape[1]} -> {features.shape[1]} (ä¿ç•™æ–¹å·®: {pca.explained_variance_ratio_.sum():.3f})")
    else:
        features = shapley_vectors.numpy()
    
    # K-meansèšç±»
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(features)
    
    return cluster_labels, kmeans

def visualize_value_distribution(value_metrics, labels, save_dir):
    """å¯è§†åŒ–æ•°æ®ä»·å€¼åˆ†å¸ƒ"""
    plt.figure(figsize=(20, 15))
    
    metrics_to_plot = ['l2_norm', 'l1_norm', 'max_value', 'min_value', 'variance', 'entropy']
    metric_titles = ['L2 Norm (Overall Value)', 'L1 Norm (Sparsity)', 'Max Value (Peak Contribution)', 
                    'Min Value (Negative Impact)', 'Variance (Inconsistency)', 'Entropy (Dispersion)']
    
    for i, (metric, title) in enumerate(zip(metrics_to_plot, metric_titles)):
        values = value_metrics[metric]
        
        # æ•´ä½“åˆ†å¸ƒ
        plt.subplot(3, 4, i*2 + 1)
        plt.hist(values, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        plt.xlabel(title)
        plt.ylabel('Frequency')
        plt.title(f'Distribution of {title}')
        plt.axvline(np.mean(values), color='red', linestyle='--', label=f'Mean: {np.mean(values):.4f}')
        plt.axvline(np.median(values), color='green', linestyle='--', label=f'Median: {np.median(values):.4f}')
        plt.legend()
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        plt.subplot(3, 4, i*2 + 2)
        unique_labels = np.unique(labels)
        values_by_class = [values[labels == label] for label in unique_labels]
        plt.boxplot(values_by_class, labels=unique_labels)
        plt.xlabel('Class')
        plt.ylabel(title)
        plt.title(f'{title} by Class')
        plt.xticks(rotation=45)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'value_distribution_analysis.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return save_path

def visualize_sample_categories(value_metrics, categories, category_names, save_dir):
    """å¯è§†åŒ–æ ·æœ¬ä»·å€¼åˆ†ç±»"""
    plt.figure(figsize=(15, 10))
    
    # åˆ†ç±»é¥¼å›¾
    plt.subplot(2, 3, 1)
    category_counts = [np.sum(categories == i) for i in range(len(category_names))]
    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(category_names)))
    plt.pie(category_counts, labels=category_names, autopct='%1.1f%%', colors=colors)
    plt.title('Sample Distribution by Value Category')
    
    # æ¯ä¸ªç±»åˆ«çš„ä»·å€¼åˆ†å¸ƒ
    plt.subplot(2, 3, 2)
    l2_norms = value_metrics['l2_norm']
    for i, name in enumerate(category_names):
        mask = categories == i
        if np.any(mask):
            plt.hist(l2_norms[mask], bins=20, alpha=0.7, label=name, color=colors[i])
    plt.xlabel('L2 Norm (Overall Value)')
    plt.ylabel('Frequency')
    plt.title('Value Distribution by Category')
    plt.legend()
    
    # æ•£ç‚¹å›¾ï¼šä¸åŒæŒ‡æ ‡çš„å…³ç³»
    plt.subplot(2, 3, 3)
    scatter = plt.scatter(value_metrics['l2_norm'], value_metrics['entropy'], 
                         c=categories, cmap='RdYlGn', alpha=0.6)
    plt.xlabel('L2 Norm (Overall Value)')
    plt.ylabel('Entropy (Dispersion)')
    plt.title('Value vs Dispersion')
    plt.colorbar(scatter, label='Value Category')
    
    # æ ·æœ¬ç´¢å¼• vs ä»·å€¼
    plt.subplot(2, 3, 4)
    plt.scatter(range(len(l2_norms)), l2_norms, c=categories, cmap='RdYlGn', alpha=0.6, s=1)
    plt.xlabel('Sample Index')
    plt.ylabel('L2 Norm (Overall Value)')
    plt.title('Sample Value Distribution')
    plt.colorbar(label='Value Category')
    
    # ç´¯ç§¯ä»·å€¼è´¡çŒ®
    plt.subplot(2, 3, 5)
    sorted_indices = np.argsort(-l2_norms)  # é™åºæ’åˆ—
    sorted_values = l2_norms[sorted_indices]
    cumulative_values = np.cumsum(sorted_values)
    cumulative_percentage = cumulative_values / cumulative_values[-1] * 100
    sample_percentage = np.arange(1, len(sorted_values) + 1) / len(sorted_values) * 100
    
    plt.plot(sample_percentage, cumulative_percentage, 'b-', linewidth=2)
    plt.axline((0, 0), slope=1, color='red', linestyle='--', alpha=0.7, label='Perfect Equality')
    plt.xlabel('Percentage of Samples (sorted by value)')
    plt.ylabel('Cumulative Percentage of Total Value')
    plt.title('Value Concentration Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ä»·å€¼ vs æ–¹å·®
    plt.subplot(2, 3, 6)
    plt.scatter(value_metrics['l2_norm'], value_metrics['variance'], 
               c=categories, cmap='RdYlGn', alpha=0.6)
    plt.xlabel('L2 Norm (Overall Value)')
    plt.ylabel('Variance (Inconsistency)')
    plt.title('Value vs Inconsistency')
    plt.colorbar(label='Value Category')
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'sample_categorization.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return save_path

def visualize_clustering_results(shapley_vectors, cluster_labels, value_metrics, save_dir):
    """å¯è§†åŒ–èšç±»ç»“æœ"""
    n_clusters = len(np.unique(cluster_labels))
    
    plt.figure(figsize=(15, 10))
    
    # ä½¿ç”¨t-SNEè¿›è¡Œå¯è§†åŒ–ï¼ˆå¦‚æœæ ·æœ¬æ•°ä¸å¤ªå¤šï¼‰
    if shapley_vectors.shape[0] <= 5000:
        plt.subplot(2, 3, 1)
        # å¦‚æœç»´åº¦å¤ªé«˜ï¼Œå…ˆPCAé™ç»´
        if shapley_vectors.shape[1] > 50:
            pca = PCA(n_components=50)
            features_for_tsne = pca.fit_transform(shapley_vectors.numpy())
        else:
            features_for_tsne = shapley_vectors.numpy()
        
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, shapley_vectors.shape[0]//4))
        tsne_result = tsne.fit_transform(features_for_tsne)
        
        scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=cluster_labels, cmap='tab10', alpha=0.6)
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')
        plt.title('Cluster Visualization (t-SNE)')
        plt.colorbar(scatter, label='Cluster')
    
    # PCAå¯è§†åŒ–
    plt.subplot(2, 3, 2)
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(shapley_vectors.numpy())
    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=cluster_labels, cmap='tab10', alpha=0.6)
    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')
    plt.title('Cluster Visualization (PCA)')
    plt.colorbar(scatter, label='Cluster')
    
    # æ¯ä¸ªèšç±»çš„ä»·å€¼åˆ†å¸ƒ
    plt.subplot(2, 3, 3)
    l2_norms = value_metrics['l2_norm']
    for cluster_id in range(n_clusters):
        mask = cluster_labels == cluster_id
        plt.hist(l2_norms[mask], bins=20, alpha=0.7, label=f'Cluster {cluster_id}')
    plt.xlabel('L2 Norm (Overall Value)')
    plt.ylabel('Frequency')
    plt.title('Value Distribution by Cluster')
    plt.legend()
    
    # èšç±»ç»Ÿè®¡
    plt.subplot(2, 3, 4)
    cluster_sizes = [np.sum(cluster_labels == i) for i in range(n_clusters)]
    plt.bar(range(n_clusters), cluster_sizes, color=plt.cm.tab10(np.arange(n_clusters)))
    plt.xlabel('Cluster ID')
    plt.ylabel('Number of Samples')
    plt.title('Cluster Sizes')
    
    # æ¯ä¸ªèšç±»çš„å¹³å‡ä»·å€¼
    plt.subplot(2, 3, 5)
    cluster_mean_values = [np.mean(l2_norms[cluster_labels == i]) for i in range(n_clusters)]
    plt.bar(range(n_clusters), cluster_mean_values, color=plt.cm.tab10(np.arange(n_clusters)))
    plt.xlabel('Cluster ID')
    plt.ylabel('Mean L2 Norm')
    plt.title('Average Value by Cluster')
    
    # èšç±»ä»·å€¼ vs å¤§å°
    plt.subplot(2, 3, 6)
    plt.scatter(cluster_sizes, cluster_mean_values, s=100, c=range(n_clusters), cmap='tab10')
    for i, (size, value) in enumerate(zip(cluster_sizes, cluster_mean_values)):
        plt.annotate(f'C{i}', (size, value), xytext=(5, 5), textcoords='offset points')
    plt.xlabel('Cluster Size')
    plt.ylabel('Mean Value')
    plt.title('Cluster Size vs Mean Value')
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'clustering_analysis.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return save_path

def generate_valuation_report(value_metrics, categories, category_names, cluster_labels, experiment_type, save_dir):
    """ç”Ÿæˆæ•°æ®ä¼°å€¼æŠ¥å‘Š"""
    report_lines = []
    report_lines.append("# æ•°æ®ä»·å€¼è¯„ä¼°æŠ¥å‘Š\n")
    report_lines.append(f"å®éªŒç±»å‹: {experiment_type.upper()}\n")
    report_lines.append(f"åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
    
    # åŸºæœ¬ç»Ÿè®¡
    total_samples = len(value_metrics['l2_norm'])
    report_lines.append("## æ•°æ®é›†æ¦‚è§ˆ\n")
    report_lines.append(f"- æ€»æ ·æœ¬æ•°: {total_samples}\n")
    report_lines.append(f"- ç‰¹å¾ç»´åº¦: {len(np.unique(cluster_labels))} ä¸ªèšç±»\n\n")
    
    # ä»·å€¼åˆ†å¸ƒç»Ÿè®¡
    l2_norms = value_metrics['l2_norm']
    report_lines.append("## ä»·å€¼åˆ†å¸ƒç»Ÿè®¡\n")
    report_lines.append(f"- å¹³å‡ä»·å€¼: {np.mean(l2_norms):.6f}\n")
    report_lines.append(f"- æ ‡å‡†å·®: {np.std(l2_norms):.6f}\n")
    report_lines.append(f"- æœ€å°å€¼: {np.min(l2_norms):.6f}\n")
    report_lines.append(f"- æœ€å¤§å€¼: {np.max(l2_norms):.6f}\n")
    report_lines.append(f"- ä¸­ä½æ•°: {np.median(l2_norms):.6f}\n\n")
    
    # åˆ†ä½æ•°åˆ†æ
    percentiles = [10, 25, 75, 90, 95, 99]
    report_lines.append("## ä»·å€¼åˆ†ä½æ•°åˆ†æ\n")
    for p in percentiles:
        value = np.percentile(l2_norms, p)
        count = np.sum(l2_norms >= value)
        report_lines.append(f"- {p}%åˆ†ä½æ•°: {value:.6f} (â‰¥æ­¤å€¼çš„æ ·æœ¬: {count}ä¸ª, {count/total_samples*100:.1f}%)\n")
    report_lines.append("\n")
    
    # ä»·å€¼ç±»åˆ«åˆ†æ
    report_lines.append("## ä»·å€¼åˆ†ç±»åˆ†æ\n")
    for i, name in enumerate(category_names):
        count = np.sum(categories == i)
        percentage = count / total_samples * 100
        mean_value = np.mean(l2_norms[categories == i])
        report_lines.append(f"- {name}: {count}ä¸ªæ ·æœ¬ ({percentage:.1f}%), å¹³å‡ä»·å€¼: {mean_value:.6f}\n")
    report_lines.append("\n")
    
    # èšç±»åˆ†æ
    n_clusters = len(np.unique(cluster_labels))
    report_lines.append("## èšç±»åˆ†æ\n")
    report_lines.append(f"- èšç±»æ•°é‡: {n_clusters}\n")
    for cluster_id in range(n_clusters):
        mask = cluster_labels == cluster_id
        size = np.sum(mask)
        mean_val = np.mean(l2_norms[mask])
        std_val = np.std(l2_norms[mask])
        report_lines.append(f"- èšç±» {cluster_id}: {size}ä¸ªæ ·æœ¬ ({size/total_samples*100:.1f}%), å¹³å‡ä»·å€¼: {mean_val:.6f}Â±{std_val:.6f}\n")
    report_lines.append("\n")
    
    # ä»·å€¼é›†ä¸­åº¦åˆ†æ
    sorted_values = np.sort(l2_norms)[::-1]
    cumsum_values = np.cumsum(sorted_values)
    total_value = cumsum_values[-1]
    
    report_lines.append("## ä»·å€¼é›†ä¸­åº¦åˆ†æ\n")
    concentration_points = [0.1, 0.2, 0.5]
    for point in concentration_points:
        idx = int(point * total_samples)
        concentrated_value = cumsum_values[idx]
        percentage = concentrated_value / total_value * 100
        report_lines.append(f"- å‰{point*100:.0f}%çš„æ ·æœ¬è´¡çŒ®äº†æ€»ä»·å€¼çš„{percentage:.1f}%\n")
    report_lines.append("\n")
    
    # å»ºè®®
    report_lines.append("## æ•°æ®ä¼˜åŒ–å»ºè®®\n")
    
    # é«˜ä»·å€¼æ ·æœ¬å»ºè®®
    high_value_threshold = np.percentile(l2_norms, 90)
    high_value_count = np.sum(l2_norms >= high_value_threshold)
    report_lines.append(f"### é«˜ä»·å€¼æ ·æœ¬ (å‰10%, {high_value_count}ä¸ªæ ·æœ¬)\n")
    report_lines.append("- âœ… è¿™äº›æ ·æœ¬å¯¹æ¨¡å‹è´¡çŒ®æœ€å¤§ï¼Œå»ºè®®ä¼˜å…ˆä¿ç•™\n")
    report_lines.append("- ğŸ’¡ å¯ä»¥åˆ†æè¿™äº›æ ·æœ¬çš„å…±åŒç‰¹å¾ï¼ŒæŒ‡å¯¼æ•°æ®æ”¶é›†ç­–ç•¥\n")
    report_lines.append("- ğŸ”„ åœ¨æ•°æ®å¢å¼ºæ—¶ï¼Œå¯ä»¥é‡ç‚¹å…³æ³¨è¿™ç±»æ ·æœ¬\n\n")
    
    # ä½ä»·å€¼æ ·æœ¬å»ºè®®
    low_value_threshold = np.percentile(l2_norms, 10)
    low_value_count = np.sum(l2_norms <= low_value_threshold)
    report_lines.append(f"### ä½ä»·å€¼æ ·æœ¬ (å10%, {low_value_count}ä¸ªæ ·æœ¬)\n")
    report_lines.append("- âš ï¸ è¿™äº›æ ·æœ¬å¯¹æ¨¡å‹è´¡çŒ®è¾ƒå°ï¼Œå¯è€ƒè™‘ç§»é™¤\n")
    report_lines.append("- ğŸ” å»ºè®®æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ ‡ç­¾é”™è¯¯æˆ–æ•°æ®è´¨é‡é—®é¢˜\n")
    report_lines.append("- ğŸ’¾ å¯ä»¥ä¼˜å…ˆä»è®­ç»ƒé›†ä¸­ç§»é™¤ä»¥å‡å°‘è®¡ç®—æˆæœ¬\n\n")
    
    # èšç±»å»ºè®®
    cluster_values = [np.mean(l2_norms[cluster_labels == i]) for i in range(n_clusters)]
    best_cluster = np.argmax(cluster_values)
    worst_cluster = np.argmin(cluster_values)
    
    report_lines.append(f"### èšç±»ç­–ç•¥å»ºè®®\n")
    report_lines.append(f"- ğŸ† èšç±»{best_cluster}ä»·å€¼æœ€é«˜ï¼Œå»ºè®®æ·±å…¥åˆ†æå…¶ç‰¹å¾æ¨¡å¼\n")
    report_lines.append(f"- ğŸ“‰ èšç±»{worst_cluster}ä»·å€¼æœ€ä½ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡\n")
    report_lines.append("- ğŸ¯ å¯ä»¥åŸºäºèšç±»ç»“æœè®¾è®¡åˆ†å±‚é‡‡æ ·ç­–ç•¥\n")
    report_lines.append("- ğŸ”„ è€ƒè™‘å¯¹ä¸åŒèšç±»é‡‡ç”¨ä¸åŒçš„æ•°æ®å¢å¼ºç­–ç•¥\n\n")
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(save_dir, f'valuation_report_{experiment_type}.md')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.writelines(report_lines)
    
    return report_path

def run_data_valuation(experiment_type='resnet', n_clusters=5):
    """è¿è¡Œå®Œæ•´çš„æ•°æ®ä»·å€¼è¯„ä¼°"""
    print(f"ğŸ’ å¼€å§‹æ•°æ®ä»·å€¼è¯„ä¼°: {experiment_type.upper()}")
    
    # åˆ›å»ºç»“æœç›®å½•
    save_dir = os.path.join(settings.RESULTS_DIR, f'data_valuation_{experiment_type}')
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½Shapleyå€¼æ•°æ®...")
    try:
        shapley_data = load_shapley_data(experiment_type)
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("è¯·å…ˆè¿è¡ŒShapleyå€¼è®¡ç®—è„šæœ¬ç”Ÿæˆæ•°æ®ã€‚")
        return
    
    shapley_vectors = shapley_data['shapley_vectors']
    labels = shapley_data['noisy_labels']
    
    # è®¡ç®—ä»·å€¼æŒ‡æ ‡
    print("ğŸ“ˆ è®¡ç®—ä»·å€¼æŒ‡æ ‡...")
    value_metrics = calculate_value_metrics(shapley_vectors)
    
    # æ ·æœ¬åˆ†ç±»
    print("ğŸ·ï¸ å¯¹æ ·æœ¬è¿›è¡Œä»·å€¼åˆ†ç±»...")
    categories, category_names, thresholds = categorize_samples_by_value(value_metrics)
    
    # èšç±»åˆ†æ
    print("ğŸ§© è¿›è¡Œèšç±»åˆ†æ...")
    cluster_labels, kmeans = cluster_samples_by_shapley(shapley_vectors, n_clusters)
    
    # å¯è§†åŒ–åˆ†æ
    print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    
    # ä»·å€¼åˆ†å¸ƒå¯è§†åŒ–
    dist_plot_path = visualize_value_distribution(value_metrics, labels, save_dir)
    
    # æ ·æœ¬åˆ†ç±»å¯è§†åŒ–
    cat_plot_path = visualize_sample_categories(value_metrics, categories, category_names, save_dir)
    
    # èšç±»ç»“æœå¯è§†åŒ–
    cluster_plot_path = visualize_clustering_results(shapley_vectors, cluster_labels, value_metrics, save_dir)
    
    # ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    report_path = generate_valuation_report(
        value_metrics, categories, category_names, cluster_labels, experiment_type, save_dir
    )
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_path = os.path.join(save_dir, 'valuation_results.pt')
    torch.save({
        'value_metrics': value_metrics,
        'categories': categories,
        'category_names': category_names,
        'cluster_labels': cluster_labels,
        'thresholds': thresholds,
        'shapley_vectors': shapley_vectors,
        'labels': labels
    }, results_path)
    
    # æ‰“å°æ‘˜è¦
    print(f"\n=== ğŸ’ æ•°æ®ä»·å€¼è¯„ä¼°ç»“æœ ===")
    print(f"æ€»æ ·æœ¬æ•°: {len(value_metrics['l2_norm'])}")
    print(f"å¹³å‡ä»·å€¼: {np.mean(value_metrics['l2_norm']):.6f}")
    print(f"ä»·å€¼æ ‡å‡†å·®: {np.std(value_metrics['l2_norm']):.6f}")
    print(f"èšç±»æ•°é‡: {n_clusters}")
    
    # åˆ†ç±»ç»Ÿè®¡
    for i, name in enumerate(category_names):
        count = np.sum(categories == i)
        percentage = count / len(categories) * 100
        print(f"{name}: {count}ä¸ªæ ·æœ¬ ({percentage:.1f}%)")
    
    print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {save_dir}")
    print(f"ğŸ“Š åˆ†å¸ƒåˆ†æå›¾: {dist_plot_path}")
    print(f"ğŸ·ï¸ åˆ†ç±»ç»“æœå›¾: {cat_plot_path}")
    print(f"ğŸ§© èšç±»åˆ†æå›¾: {cluster_plot_path}")
    print(f"ğŸ“ è¯„ä¼°æŠ¥å‘Š: {report_path}")
    print(f"ğŸ’¾ è¯¦ç»†ç»“æœ: {results_path}")
    
    return {
        'value_metrics': value_metrics,
        'categories': categories,
        'category_names': category_names,
        'cluster_labels': cluster_labels,
        'save_dir': save_dir,
        'plots': {
            'distribution': dist_plot_path,
            'categorization': cat_plot_path,
            'clustering': cluster_plot_path
        },
        'report': report_path,
        'results_file': results_path
    }

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="åŸºäºShapleyå€¼çš„æ•°æ®ä»·å€¼è¯„ä¼°")
    parser.add_argument("--type", choices=["resnet", "transformer"], 
                       default="resnet", help="å®éªŒç±»å‹")
    parser.add_argument("--clusters", type=int, default=5,
                       help="èšç±»æ•°é‡ (default: 5)")
    args = parser.parse_args()
    
    run_data_valuation(args.type, args.clusters)