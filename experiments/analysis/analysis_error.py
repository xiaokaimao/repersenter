"""
åŸºäºShapleyå€¼çš„é”™è¯¯æ£€æµ‹å’Œæ•°æ®è´¨é‡åˆ†æ
ç”¨äºè¯†åˆ«æ ‡ç­¾å™ªå£°ã€å¼‚å¸¸æ ·æœ¬å’Œä½è´¨é‡æ•°æ®
"""
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.manifold import TSNE
import pandas as pd

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
import config.settings as settings

def load_shapley_data(experiment_type='resnet'):
    """åŠ è½½Shapleyå€¼æ•°æ®"""
    config = settings.CORE_SET_EXPERIMENT_CONFIG[experiment_type]
    
    if experiment_type == 'transformer':
        model_name_safe = config['model_name'].replace('/', '_')
        shapley_file_name = f"shapley_vectors_{config['dataset_name']}_{model_name_safe}_noise_{config.get('label_noise_rate', 0.0)}.pt"
    else:
        shapley_file_name = f"shapley_vectors_{config['dataset_name']}_{config['model_name']}_noise_{config.get('label_noise_rate', 0.0)}.pt"
    
    load_path = os.path.join(settings.RESULTS_DIR, shapley_file_name)
    
    if not os.path.exists(load_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°Shapleyæ–‡ä»¶: {load_path}")
    
    return torch.load(load_path, map_location='cpu')

def calculate_usefulness_scores(shapley_vectors):
    """è®¡ç®—æ ·æœ¬æœ‰ç”¨æ€§åˆ†æ•°"""
    # ä½¿ç”¨L2èŒƒæ•°ä½œä¸ºæœ‰ç”¨æ€§æŒ‡æ ‡
    usefulness_scores = torch.linalg.norm(shapley_vectors, dim=1).numpy()
    return usefulness_scores

def detect_mislabeled_samples(shapley_data, threshold_percentile=5):
    """
    æ£€æµ‹å¯èƒ½çš„é”™è¯¯æ ‡ç­¾æ ·æœ¬
    
    Args:
        shapley_data: åŒ…å«Shapleyå€¼å’Œæ ‡ç­¾ä¿¡æ¯çš„æ•°æ®
        threshold_percentile: ä½äºæ­¤ç™¾åˆ†ä½çš„æ ·æœ¬è¢«è®¤ä¸ºæ˜¯å¯ç–‘çš„
    
    Returns:
        dict: åŒ…å«æ£€æµ‹ç»“æœçš„å­—å…¸
    """
    shapley_vectors = shapley_data['shapley_vectors']
    noisy_labels = shapley_data['noisy_labels']
    original_labels = shapley_data.get('original_labels', noisy_labels)
    flipped_indices = shapley_data.get('flipped_indices', [])
    
    usefulness_scores = calculate_usefulness_scores(shapley_vectors)
    
    # è®¡ç®—é˜ˆå€¼
    threshold = np.percentile(usefulness_scores, threshold_percentile)
    
    # è¯†åˆ«ä½æœ‰ç”¨æ€§æ ·æœ¬
    suspicious_indices = np.where(usefulness_scores < threshold)[0]
    
    # å¦‚æœæœ‰çœŸå®çš„ç¿»è½¬ä¿¡æ¯ï¼Œè®¡ç®—æ£€æµ‹å‡†ç¡®ç‡
    detection_results = {
        'suspicious_indices': suspicious_indices,
        'usefulness_scores': usefulness_scores,
        'threshold': threshold,
        'num_suspicious': len(suspicious_indices)
    }
    
    if len(flipped_indices) > 0:
        # è®¡ç®—æ£€æµ‹æ€§èƒ½
        true_flipped = set(flipped_indices)
        detected_suspicious = set(suspicious_indices)
        
        true_positives = len(true_flipped & detected_suspicious)
        false_positives = len(detected_suspicious - true_flipped)
        false_negatives = len(true_flipped - detected_suspicious)
        true_negatives = len(usefulness_scores) - len(true_flipped) - false_positives
        
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        detection_results.update({
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'true_positives': true_positives,
            'false_positives': false_positives,
            'false_negatives': false_negatives,
            'actual_flipped_count': len(flipped_indices)
        })
    
    return detection_results

def analyze_shapley_distribution(shapley_vectors, labels, save_dir):
    """åˆ†æShapleyå€¼çš„åˆ†å¸ƒ"""
    usefulness_scores = calculate_usefulness_scores(shapley_vectors)
    
    # åˆ›å»ºåˆ†å¸ƒå›¾
    plt.figure(figsize=(15, 10))
    
    # å­å›¾1: æ•´ä½“åˆ†å¸ƒç›´æ–¹å›¾
    plt.subplot(2, 3, 1)
    plt.hist(usefulness_scores, bins=50, alpha=0.7, color='skyblue')
    plt.xlabel('Shapley Value Norm')
    plt.ylabel('Frequency')
    plt.title('Distribution of Shapley Value Norms')
    plt.axvline(np.percentile(usefulness_scores, 5), color='red', linestyle='--', label='5th percentile')
    plt.axvline(np.percentile(usefulness_scores, 95), color='green', linestyle='--', label='95th percentile')
    plt.legend()
    
    # å­å›¾2: æŒ‰ç±»åˆ«åˆ†ç»„çš„ç®±çº¿å›¾
    plt.subplot(2, 3, 2)
    unique_labels = np.unique(labels)
    scores_by_class = [usefulness_scores[labels == label] for label in unique_labels]
    plt.boxplot(scores_by_class, labels=unique_labels)
    plt.xlabel('Class')
    plt.ylabel('Shapley Value Norm')
    plt.title('Shapley Values by Class')
    
    # å­å›¾3: ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
    plt.subplot(2, 3, 3)
    sorted_scores = np.sort(usefulness_scores)
    cumulative = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores)
    plt.plot(sorted_scores, cumulative)
    plt.xlabel('Shapley Value Norm')
    plt.ylabel('Cumulative Probability')
    plt.title('Cumulative Distribution')
    plt.grid(True, alpha=0.3)
    
    # å­å›¾4: æ¯ä¸ªç±»åˆ«çš„å¹³å‡Shapleyå€¼
    plt.subplot(2, 3, 4)
    class_means = [np.mean(usefulness_scores[labels == label]) for label in unique_labels]
    plt.bar(unique_labels, class_means, color='lightcoral')
    plt.xlabel('Class')
    plt.ylabel('Mean Shapley Value Norm')
    plt.title('Mean Shapley Values by Class')
    
    # å­å›¾5: æ•£ç‚¹å›¾ - æ ·æœ¬ç´¢å¼• vs Shapleyå€¼
    plt.subplot(2, 3, 5)
    plt.scatter(range(len(usefulness_scores)), usefulness_scores, alpha=0.6, s=1)
    plt.xlabel('Sample Index')
    plt.ylabel('Shapley Value Norm')
    plt.title('Shapley Values vs Sample Index')
    
    # å­å›¾6: çƒ­åŠ›å›¾ - Shapleyå‘é‡çš„å‰å‡ ä¸ªç»´åº¦
    plt.subplot(2, 3, 6)
    if shapley_vectors.shape[1] > 1:
        # é€‰æ‹©å‰10ä¸ªç»´åº¦æˆ–æ‰€æœ‰ç»´åº¦ï¼ˆå¦‚æœå°‘äº10ä¸ªï¼‰
        dims_to_show = min(10, shapley_vectors.shape[1])
        sample_indices = np.random.choice(shapley_vectors.shape[0], min(100, shapley_vectors.shape[0]), replace=False)
        heatmap_data = shapley_vectors[sample_indices, :dims_to_show].numpy()
        sns.heatmap(heatmap_data, cmap='coolwarm', center=0, cbar_kws={'label': 'Shapley Value'})
        plt.xlabel('Shapley Dimension')
        plt.ylabel('Sample (Random Selection)')
        plt.title('Shapley Vector Heatmap')
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'shapley_distribution_analysis.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return save_path

def visualize_detection_results(detection_results, shapley_data, save_dir):
    """å¯è§†åŒ–é”™è¯¯æ£€æµ‹ç»“æœ"""
    usefulness_scores = detection_results['usefulness_scores']
    suspicious_indices = detection_results['suspicious_indices']
    threshold = detection_results['threshold']
    
    # è·å–æ ‡ç­¾ä¿¡æ¯
    flipped_indices = shapley_data.get('flipped_indices', [])
    
    plt.figure(figsize=(15, 10))
    
    # å­å›¾1: æ£€æµ‹ç»“æœæ•£ç‚¹å›¾
    plt.subplot(2, 3, 1)
    # æ­£å¸¸æ ·æœ¬
    normal_mask = np.ones(len(usefulness_scores), dtype=bool)
    normal_mask[suspicious_indices] = False
    plt.scatter(np.where(normal_mask)[0], usefulness_scores[normal_mask], 
               alpha=0.6, s=10, color='green', label='Normal Samples')
    
    # å¯ç–‘æ ·æœ¬
    plt.scatter(suspicious_indices, usefulness_scores[suspicious_indices], 
               alpha=0.8, s=15, color='red', label='Suspicious Samples')
    
    # å¦‚æœæœ‰çœŸå®ç¿»è½¬ä¿¡æ¯ï¼Œæ ‡è®°å‡ºæ¥
    if len(flipped_indices) > 0:
        plt.scatter(flipped_indices, usefulness_scores[flipped_indices], 
                   alpha=0.8, s=20, color='orange', marker='x', label='Actually Flipped')
    
    plt.axhline(threshold, color='black', linestyle='--', label=f'Threshold ({threshold:.4f})')
    plt.xlabel('Sample Index')
    plt.ylabel('Shapley Value Norm')
    plt.title('Mislabel Detection Results')
    plt.legend()
    
    # å­å›¾2: æ£€æµ‹æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼‰
    if 'precision' in detection_results:
        plt.subplot(2, 3, 2)
        metrics = ['Precision', 'Recall', 'F1-Score']
        values = [detection_results['precision'], detection_results['recall'], detection_results['f1_score']]
        bars = plt.bar(metrics, values, color=['skyblue', 'lightgreen', 'coral'])
        plt.ylabel('Score')
        plt.title('Detection Performance Metrics')
        plt.ylim(0, 1)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar, value in zip(bars, values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{value:.3f}', ha='center', va='bottom')
    
    # å­å›¾3: æ··æ·†çŸ©é˜µï¼ˆå¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼‰
    if len(flipped_indices) > 0:
        plt.subplot(2, 3, 3)
        
        # åˆ›å»ºçœŸå®æ ‡ç­¾ï¼ˆ0=æ­£å¸¸ï¼Œ1=ç¿»è½¬ï¼‰å’Œé¢„æµ‹æ ‡ç­¾ï¼ˆ0=æ­£å¸¸ï¼Œ1=å¯ç–‘ï¼‰
        y_true = np.zeros(len(usefulness_scores))
        y_true[flipped_indices] = 1
        
        y_pred = np.zeros(len(usefulness_scores))
        y_pred[suspicious_indices] = 1
        
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Normal', 'Suspicious'],
                   yticklabels=['Normal', 'Flipped'])
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title('Confusion Matrix')
    
    # å­å›¾4: æœ‰ç”¨æ€§åˆ†æ•°åˆ†å¸ƒï¼ˆæ­£å¸¸ vs å¯ç–‘ï¼‰
    plt.subplot(2, 3, 4)
    normal_scores = usefulness_scores[normal_mask]
    suspicious_scores = usefulness_scores[suspicious_indices]
    
    plt.hist(normal_scores, bins=30, alpha=0.7, label='Normal', color='green', density=True)
    plt.hist(suspicious_scores, bins=30, alpha=0.7, label='Suspicious', color='red', density=True)
    plt.axvline(threshold, color='black', linestyle='--', label='Threshold')
    plt.xlabel('Shapley Value Norm')
    plt.ylabel('Density')
    plt.title('Score Distribution: Normal vs Suspicious')
    plt.legend()
    
    # å­å›¾5: ROCæ›²çº¿ï¼ˆå¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼‰
    if len(flipped_indices) > 0:
        plt.subplot(2, 3, 5)
        from sklearn.metrics import roc_curve, auc
        
        # ä½¿ç”¨è´Ÿçš„æœ‰ç”¨æ€§åˆ†æ•°ä½œä¸º"å¼‚å¸¸åˆ†æ•°"ï¼ˆè¶Šä½è¶Šå¯ç–‘ï¼‰
        y_scores = -usefulness_scores
        fpr, tpr, _ = roc_curve(y_true, y_scores)
        roc_auc = auc(fpr, tpr)
        
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
    
    # å­å›¾6: æ ·æœ¬æ’åºå›¾
    plt.subplot(2, 3, 6)
    sorted_indices = np.argsort(usefulness_scores)
    sorted_scores = usefulness_scores[sorted_indices]
    
    plt.plot(range(len(sorted_scores)), sorted_scores, color='blue', alpha=0.7)
    
    # æ ‡è®°å¯ç–‘æ ·æœ¬çš„ä½ç½®
    suspicious_positions = np.where(np.isin(sorted_indices, suspicious_indices))[0]
    plt.scatter(suspicious_positions, sorted_scores[suspicious_positions], 
               color='red', s=10, alpha=0.8, label='Suspicious')
    
    # å¦‚æœæœ‰çœŸå®ç¿»è½¬ä¿¡æ¯
    if len(flipped_indices) > 0:
        flipped_positions = np.where(np.isin(sorted_indices, flipped_indices))[0]
        plt.scatter(flipped_positions, sorted_scores[flipped_positions], 
                   color='orange', s=15, marker='x', alpha=0.8, label='Actually Flipped')
    
    plt.axhline(threshold, color='black', linestyle='--', label='Threshold')
    plt.xlabel('Sample Rank (by Shapley Value)')
    plt.ylabel('Shapley Value Norm')
    plt.title('Samples Sorted by Usefulness')
    plt.legend()
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'mislabel_detection_results.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return save_path

def generate_analysis_report(detection_results, experiment_type, save_dir):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    report_lines = []
    report_lines.append("# Shapleyå€¼é”™è¯¯æ£€æµ‹åˆ†ææŠ¥å‘Š\n")
    report_lines.append(f"å®éªŒç±»å‹: {experiment_type.upper()}\n")
    report_lines.append(f"åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
    
    report_lines.append("## æ£€æµ‹ç»“æœæ¦‚è¿°\n")
    report_lines.append(f"- æ€»æ ·æœ¬æ•°: {len(detection_results['usefulness_scores'])}\n")
    report_lines.append(f"- å¯ç–‘æ ·æœ¬æ•°: {detection_results['num_suspicious']}\n")
    report_lines.append(f"- å¯ç–‘æ ·æœ¬æ¯”ä¾‹: {detection_results['num_suspicious']/len(detection_results['usefulness_scores'])*100:.2f}%\n")
    report_lines.append(f"- æ£€æµ‹é˜ˆå€¼: {detection_results['threshold']:.6f}\n\n")
    
    if 'precision' in detection_results:
        report_lines.append("## æ£€æµ‹æ€§èƒ½æŒ‡æ ‡\n")
        report_lines.append(f"- ç²¾ç¡®ç‡ (Precision): {detection_results['precision']:.4f}\n")
        report_lines.append(f"- å¬å›ç‡ (Recall): {detection_results['recall']:.4f}\n")
        report_lines.append(f"- F1åˆ†æ•°: {detection_results['f1_score']:.4f}\n")
        report_lines.append(f"- çœŸæ­£ä¾‹ (TP): {detection_results['true_positives']}\n")
        report_lines.append(f"- å‡æ­£ä¾‹ (FP): {detection_results['false_positives']}\n")
        report_lines.append(f"- å‡è´Ÿä¾‹ (FN): {detection_results['false_negatives']}\n")
        report_lines.append(f"- å®é™…ç¿»è½¬æ ·æœ¬æ•°: {detection_results['actual_flipped_count']}\n\n")
    
    # ç»Ÿè®¡ä¿¡æ¯
    scores = detection_results['usefulness_scores']
    report_lines.append("## æœ‰ç”¨æ€§åˆ†æ•°ç»Ÿè®¡\n")
    report_lines.append(f"- å¹³å‡å€¼: {np.mean(scores):.6f}\n")
    report_lines.append(f"- æ ‡å‡†å·®: {np.std(scores):.6f}\n")
    report_lines.append(f"- æœ€å°å€¼: {np.min(scores):.6f}\n")
    report_lines.append(f"- æœ€å¤§å€¼: {np.max(scores):.6f}\n")
    report_lines.append(f"- 5%åˆ†ä½æ•°: {np.percentile(scores, 5):.6f}\n")
    report_lines.append(f"- 95%åˆ†ä½æ•°: {np.percentile(scores, 95):.6f}\n\n")
    
    report_lines.append("## å»ºè®®\n")
    if 'f1_score' in detection_results:
        if detection_results['f1_score'] > 0.7:
            report_lines.append("- âœ… æ£€æµ‹æ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥ä¿¡ä»»æ£€æµ‹ç»“æœ\n")
        elif detection_results['f1_score'] > 0.5:
            report_lines.append("- âš ï¸ æ£€æµ‹æ€§èƒ½ä¸­ç­‰ï¼Œå»ºè®®ç»“åˆå…¶ä»–æ–¹æ³•éªŒè¯\n")
        else:
            report_lines.append("- âŒ æ£€æµ‹æ€§èƒ½è¾ƒå·®ï¼Œå»ºè®®è°ƒæ•´é˜ˆå€¼æˆ–æ–¹æ³•\n")
    
    report_lines.append("- å»ºè®®æ‰‹åŠ¨æ£€æŸ¥æœ‰ç”¨æ€§åˆ†æ•°æœ€ä½çš„æ ·æœ¬\n")
    report_lines.append("- å¯ä»¥è€ƒè™‘ç§»é™¤æˆ–é‡æ–°æ ‡æ³¨æ£€æµ‹åˆ°çš„å¯ç–‘æ ·æœ¬\n")
    report_lines.append("- ç›‘æ§æ•°æ®è´¨é‡ï¼Œå®šæœŸè¿›è¡Œé”™è¯¯æ£€æµ‹åˆ†æ\n")
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(save_dir, f'analysis_report_{experiment_type}.md')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.writelines(report_lines)
    
    return report_path

def run_error_analysis(experiment_type='resnet', threshold_percentile=5):
    """è¿è¡Œå®Œæ•´çš„é”™è¯¯åˆ†æ"""
    print(f"ğŸ” å¼€å§‹é”™è¯¯åˆ†æ: {experiment_type.upper()}")
    
    # åˆ›å»ºç»“æœç›®å½•
    save_dir = os.path.join(settings.RESULTS_DIR, f'error_analysis_{experiment_type}')
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½Shapleyå€¼æ•°æ®...")
    try:
        shapley_data = load_shapley_data(experiment_type)
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("è¯·å…ˆè¿è¡ŒShapleyå€¼è®¡ç®—è„šæœ¬ç”Ÿæˆæ•°æ®ã€‚")
        return
    
    # æ£€æµ‹é”™è¯¯æ ‡ç­¾
    print("ğŸ•µï¸ æ£€æµ‹å¯ç–‘æ ·æœ¬...")
    detection_results = detect_mislabeled_samples(shapley_data, threshold_percentile)
    
    # åˆ†æShapleyåˆ†å¸ƒ
    print("ğŸ“ˆ åˆ†æShapleyå€¼åˆ†å¸ƒ...")
    dist_plot_path = analyze_shapley_distribution(
        shapley_data['shapley_vectors'], 
        shapley_data['noisy_labels'],
        save_dir
    )
    
    # å¯è§†åŒ–æ£€æµ‹ç»“æœ
    print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    detection_plot_path = visualize_detection_results(detection_results, shapley_data, save_dir)
    
    # ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    report_path = generate_analysis_report(detection_results, experiment_type, save_dir)
    
    # æ‰“å°ç»“æœ
    print(f"\n=== ğŸ¯ é”™è¯¯åˆ†æç»“æœ ===")
    print(f"æ€»æ ·æœ¬æ•°: {len(detection_results['usefulness_scores'])}")
    print(f"å¯ç–‘æ ·æœ¬æ•°: {detection_results['num_suspicious']}")
    print(f"å¯ç–‘æ ·æœ¬æ¯”ä¾‹: {detection_results['num_suspicious']/len(detection_results['usefulness_scores'])*100:.2f}%")
    
    if 'f1_score' in detection_results:
        print(f"æ£€æµ‹ç²¾ç¡®ç‡: {detection_results['precision']:.4f}")
        print(f"æ£€æµ‹å¬å›ç‡: {detection_results['recall']:.4f}")
        print(f"F1åˆ†æ•°: {detection_results['f1_score']:.4f}")
    
    print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {save_dir}")
    print(f"ğŸ“Š åˆ†å¸ƒåˆ†æå›¾: {dist_plot_path}")
    print(f"ğŸ¨ æ£€æµ‹ç»“æœå›¾: {detection_plot_path}")
    print(f"ğŸ“ åˆ†ææŠ¥å‘Š: {report_path}")
    
    return {
        'detection_results': detection_results,
        'save_dir': save_dir,
        'plots': {
            'distribution': dist_plot_path,
            'detection': detection_plot_path
        },
        'report': report_path
    }

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="åŸºäºShapleyå€¼çš„é”™è¯¯æ£€æµ‹åˆ†æ")
    parser.add_argument("--type", choices=["resnet", "transformer"], 
                       default="resnet", help="å®éªŒç±»å‹")
    parser.add_argument("--threshold", type=float, default=5.0,
                       help="æ£€æµ‹é˜ˆå€¼ç™¾åˆ†ä½æ•° (default: 5.0)")
    args = parser.parse_args()
    
    run_error_analysis(args.type, args.threshold)