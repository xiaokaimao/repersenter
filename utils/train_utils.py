"""
ç®€å•çš„å¤šå¡è®­ç»ƒå·¥å…·å‡½æ•°
"""
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
from accelerate import Accelerator, DistributedDataParallelKwargs
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.data_utils import set_seed, get_transformer_model, get_model
import models
import config.settings as settings

def train_model_with_accelerate(
    train_loader, 
    test_loader, 
    model_config, 
    num_classes, 
    experiment_name="Training",
    model_type="transformer",  # "transformer" æˆ– "resnet"
    save_model=True  # æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹
):
    """
    ç®€å•çš„å¤šå¡è®­ç»ƒå‡½æ•° - æ”¯æŒ Transformer å’Œ ResNet
    
    Args:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨  
        model_config: æ¨¡å‹é…ç½®å­—å…¸ï¼ŒåŒ…å«:
            - model_name: æ¨¡å‹åç§°
            - learning_rate: å­¦ä¹ ç‡
            - lambda_reg: L2æ­£åˆ™åŒ–
            - num_epochs: è®­ç»ƒè½®æ•°
            - use_bf16: æ˜¯å¦ä½¿ç”¨bf16 (å¯é€‰)
        num_classes: ç±»åˆ«æ•°
        experiment_name: å®éªŒåç§°
        model_type: "transformer" æˆ– "resnet"
        save_model: æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹
    
    Returns:
        history: è®­ç»ƒå†å² {'epoch': [...], 'accuracy': [...]}
    """
    # åˆå§‹åŒ– Accelerator
    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=False)
    accelerator = Accelerator(
        gradient_accumulation_steps=model_config.get("gradient_accumulation_steps", 1),
        kwargs_handlers=[ddp_kwargs]
    )
    
    if accelerator.is_main_process:
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {experiment_name}")
        print(f"æ¨¡å‹ç±»å‹: {model_type}")
        print(f"æ¨¡å‹: {model_config['model_name']}")
        print(f"GPUæ•°é‡: {accelerator.num_processes}")
    
    set_seed(settings.SEED)
    
    # åˆ›å»ºæ¨¡å‹
    if model_type == "transformer":
        model = get_transformer_model(
            model_name=model_config['model_name'],
            num_classes=num_classes,
            use_bf16=model_config.get('use_bf16', False)
        )
        # Transformeråªè®­ç»ƒåˆ†ç±»å¤´
        trainable_params = [p for p in model.parameters() if p.requires_grad]
    else:
        # ResNet
        model = get_model(
            model_name=model_config['model_name'],
            num_classes=num_classes,
            from_scratch=True
        )
        # ResNetåˆ†å±‚è®¾ç½®æ­£åˆ™åŒ–
        classifier_keywords = ['fc', 'linear', 'classifier']
        classifier_params = []
        base_params = []
        for name, param in model.named_parameters():
            if any(kw in name for kw in classifier_keywords):
                classifier_params.append(param)
            else:
                base_params.append(param)
        trainable_params = [
            {'params': base_params, 'weight_decay': 0},
            {'params': classifier_params, 'weight_decay': model_config['lambda_reg']}
        ]
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    if model_type == "transformer":
        optimizer = optim.AdamW(
            trainable_params,
            lr=model_config['learning_rate'],
            weight_decay=model_config['lambda_reg']
        )
    else:
        optimizer = optim.AdamW(trainable_params, lr=model_config['learning_rate'])
    
    criterion = nn.CrossEntropyLoss()
    
    if accelerator.is_main_process:
        num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"å¯è®­ç»ƒå‚æ•°: {num_trainable / 1e6:.2f}M")
    
    # Accelerator prepare
    model, optimizer, train_loader, test_loader, criterion = accelerator.prepare(
        model, optimizer, train_loader, test_loader, criterion
    )
    
    # è®­ç»ƒå¾ªç¯
    history = {'epoch': [], 'accuracy': []}
    best_accuracy = 0.0
    
    # è®¾ç½®ä¿å­˜è·¯å¾„
    model_save_path = None
    if save_model and accelerator.is_main_process:
        output_dir = settings.CHECKPOINT_DIR
        os.makedirs(output_dir, exist_ok=True)
        
        if model_type == "transformer":
            model_name_safe = model_config['model_name'].replace('/', '_')
            model_save_path = os.path.join(
                output_dir, 
                f"finetuned_{model_name_safe}_on_{model_config.get('dataset_name', 'unknown')}_lambda_{model_config['lambda_reg']}_noise_{model_config.get('label_noise_rate', 0.0)}.pth"
            )
        else:
            model_save_path = os.path.join(
                output_dir, 
                f"{model_config['model_name']}_on_{model_config.get('dataset_name', 'unknown')}_lambda_{model_config['lambda_reg']}_noise_{model_config.get('label_noise_rate', 0.0)}.pth"
            )
        
        print(f"æ¨¡å‹å°†ä¿å­˜åˆ°: {model_save_path}")
    
    for epoch in range(model_config["num_epochs"]):
        # è®­ç»ƒ
        model.train()
        running_loss = 0.0
        num_batches = 0
        
        train_progress = tqdm(
            train_loader, 
            desc=f"Epoch {epoch+1} Training", 
            disable=not accelerator.is_main_process
        )
        
        for batch in train_progress:
            with accelerator.accumulate(model):
                if model_type == "transformer":
                    # Transformeræ•°æ®æ ¼å¼
                    inputs = {k: v for k, v in batch.items() if k != 'labels'}
                    labels = batch['labels']
                    with accelerator.autocast():
                        outputs = model(**inputs)
                        loss = criterion(outputs.logits, labels)
                else:
                    # ResNetæ•°æ®æ ¼å¼
                    inputs, labels = batch
                    with accelerator.autocast():
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)
                
                running_loss += loss.item()
                num_batches += 1
                
                accelerator.backward(loss)
                
                if accelerator.sync_gradients:
                    optimizer.step()
                    optimizer.zero_grad()
                
                train_progress.set_postfix(loss=f"{loss.item():.4f}")
        
        # è¯„ä¼°
        model.eval()
        correct = 0
        total = 0
        
        eval_progress = tqdm(
            test_loader, 
            desc=f"Epoch {epoch+1} Evaluating", 
            disable=not accelerator.is_main_process
        )
        
        with torch.no_grad():
            for batch in eval_progress:
                if model_type == "transformer":
                    inputs = {k: v for k, v in batch.items() if k != 'labels'}
                    labels = batch['labels']
                    with accelerator.autocast():
                        outputs = model(**inputs)
                    predictions = outputs.logits.argmax(dim=-1)
                else:
                    inputs, labels = batch
                    with accelerator.autocast():
                        outputs = model(inputs)
                    predictions = outputs.argmax(dim=-1)
                
                predictions, labels = accelerator.gather_for_metrics((predictions, labels))
                correct += (predictions == labels).sum().item()
                total += labels.size(0)
        
        val_acc = 100 * correct / total
        train_loss = running_loss / num_batches if num_batches > 0 else 0.0
        
        if accelerator.is_main_process:
            print(f"Epoch {epoch+1}/{model_config['num_epochs']} | "
                  f"Train Loss: {train_loss:.4f} | "
                  f"Val Acc: {val_acc:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if save_model and val_acc > best_accuracy:
                best_accuracy = val_acc
                unwrapped_model = accelerator.unwrap_model(model)
                accelerator.save(unwrapped_model.state_dict(), model_save_path)
                print(f"ğŸ“€ æ¨¡å‹å·²ä¿å­˜ (å‡†ç¡®ç‡: {best_accuracy:.2f}%): {model_save_path}")
        
        history['epoch'].append(epoch + 1)
        history['accuracy'].append(val_acc)
    
    if accelerator.is_main_process:
        print(f"âœ… {experiment_name} å®Œæˆ")
        print(f"æœ€ç»ˆå‡†ç¡®ç‡: {val_acc:.2f}%")
        if save_model:
            print(f"ğŸ† æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")
            print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_save_path}")
    
    return history