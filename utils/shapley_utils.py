#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„Shapleyå€¼è®¡ç®—å·¥å…·
åŒ…å«æ ¸å¿ƒç®—æ³•å®ç°å’Œä¾¿æ·çš„ä½¿ç”¨æ¥å£
æ”¯æŒResNetå’ŒTransformeræ¨¡å‹ï¼Œæ”¯æŒå•GPUå’Œå¤šGPUåŠ é€Ÿ
"""

import torch
import torch.nn as nn
import os
import sys
from tqdm import tqdm
from accelerate import Accelerator

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import utils
import config.settings as settings

# ================================
# æ ¸å¿ƒç®—æ³•å®ç°
# ================================

class ModelWrapper(nn.Module):
    """
    ç»Ÿä¸€çš„æ¨¡å‹åŒ…è£…å™¨ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­åŒæ—¶è¿”å›logitså’Œfeaturesã€‚
    æ”¯æŒResNetå’ŒTransformeræ¨¡å‹ã€‚
    """
    def __init__(self, model, model_type='resnet'):
        super().__init__()
        self.model = model
        self.model_type = model_type
        
        if model_type == 'transformer':
            # Transformeræ¨¡å‹çš„åŸºç¡€éƒ¨åˆ†ä½œä¸ºç‰¹å¾æå–å™¨
            # Hugging Face AutoModelForSequenceClassification é€šå¸¸å°†åŸºç¡€æ¨¡å‹å‘½åä¸ºå„ç§åç§°
            if hasattr(model, 'transformer'): 
                self.base_model = model.transformer
            elif hasattr(model, 'roberta'): 
                self.base_model = model.roberta
            elif hasattr(model, 'bert'): 
                self.base_model = model.bert
            elif hasattr(model, 'base_model'): 
                self.base_model = model.base_model
            else: 
                raise ValueError("æ— æ³•è‡ªåŠ¨ç¡®å®šTransformerçš„åŸºç¡€æ¨¡å‹ã€‚")
            
            # åˆ†ç±»å¤´é€šå¸¸æ˜¯ 'classifier' æˆ– 'score'
            if hasattr(model, 'classifier'):
                self.classifier = model.classifier
            elif hasattr(model, 'score'):
                self.classifier = model.score
            else:
                raise ValueError("æ— æ³•è‡ªåŠ¨ç¡®å®šTransformerçš„åˆ†ç±»å¤´ã€‚")
                
        else: # for ResNet and other vision models
            # è‡ªåŠ¨æ£€æµ‹åˆ†ç±»å±‚
            if hasattr(model, 'fc'):
                self.classifier = model.fc
                # åˆ›å»ºç‰¹å¾æå–å™¨ï¼ˆé™¤äº†æœ€åçš„åˆ†ç±»å±‚ï¼‰
                self.feature_extractor = nn.Sequential(*list(model.children())[:-1])
            elif hasattr(model, 'classifier'):
                self.classifier = model.classifier
                self.feature_extractor = nn.Sequential(*list(model.children())[:-1])
            else:
                # å¦‚æœæ‰¾ä¸åˆ°æ ‡å‡†çš„åˆ†ç±»å±‚ï¼Œå°è¯•æœ€åä¸€ä¸ªLinearå±‚
                for name, module in reversed(list(model.named_modules())):
                    if isinstance(module, nn.Linear):
                        self.classifier = module
                        # åˆ›å»ºä¸åŒ…å«åˆ†ç±»å±‚çš„ç‰¹å¾æå–å™¨
                        layers = []
                        for child_name, child in model.named_children():
                            if child_name != name.split('.')[0]:  # ä¸åŒ…å«åˆ†ç±»å±‚çš„éƒ¨åˆ†
                                layers.append(child)
                        self.feature_extractor = nn.Sequential(*layers)
                        break
                else:
                    raise ValueError("æ— æ³•è‡ªåŠ¨ç¡®å®šResNetçš„åˆ†ç±»å±‚")
            
            # æ·»åŠ å…¨å±€å¹³å‡æ± åŒ–ä»¥ç¡®ä¿è¾“å‡ºç»´åº¦æ­£ç¡®
            self.global_pool = nn.AdaptiveAvgPool2d((1, 1))

    def forward(self, inputs):
        if self.model_type == 'transformer':
            # å¯¹äºTransformerï¼Œinputsæ˜¯ä¸€ä¸ªå­—å…¸
            base_outputs = self.base_model(**inputs)
            # ä½¿ç”¨æœ€åä¸€å±‚ hidden state çš„å¹³å‡æ± åŒ–ä½œä¸ºç‰¹å¾
            features = base_outputs.last_hidden_state.mean(dim=1)
            # å°†ç‰¹å¾è¾“å…¥åˆ†ç±»å¤´å¾—åˆ°logits
            logits = self.classifier(features)
        else:
            # å¯¹äºResNetï¼Œinputsæ˜¯å¼ é‡
            x = self.feature_extractor(inputs)
            # ç¡®ä¿ç‰¹å¾æ˜¯2Dçš„ï¼ˆbatch_size, feature_dimï¼‰
            if len(x.shape) > 2:
                x = self.global_pool(x)
            features = x.view(x.size(0), -1)
            logits = self.classifier(features)
        return logits, features

def calculate_shapley_vectors_single_gpu(model, train_loader, test_loader, num_classes, device, lambda_reg, num_test_samples_to_use=-1, model_type='resnet'):
    """
    ä¼˜åŒ–åçš„å•GPUç‰ˆæœ¬Shapleyè®¡ç®—
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        num_classes: ç±»åˆ«æ•°é‡
        device: è®¡ç®—è®¾å¤‡
        lambda_reg: æ­£åˆ™åŒ–å‚æ•°
        num_test_samples_to_use: ä½¿ç”¨çš„æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆ-1è¡¨ç¤ºå…¨éƒ¨ï¼‰
        model_type: æ¨¡å‹ç±»å‹ ('resnet' æˆ– 'transformer')
    
    Returns:
        tuple: (shapley_vectors, train_labels)
    """
    # ä½¿ç”¨ModelWrapper
    wrapper = ModelWrapper(model, model_type).to(device)
    wrapper.eval()
    
    # --- æ­¥éª¤ 1: é¢„è®¡ç®—æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„å±æ€§ ---
    print("--- Step 1/4: Extracting features and logits...")
    all_features, all_logits, all_labels = [], [], []
    with torch.no_grad():
        for batch in tqdm(train_loader, desc="Extracting train data"):
            if model_type == 'transformer':
                # å¤„ç†æ–‡æœ¬æ•°æ®
                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                labels = batch['labels']
            else:
                # å¤„ç†å›¾åƒæ•°æ®
                inputs, labels = batch
                inputs = inputs.to(device)
            
            logits, features = wrapper(inputs)
            
            all_features.append(features.cpu())
            all_logits.append(logits.cpu())
            all_labels.append(labels.cpu())     
    
    F_train = torch.cat(all_features, dim=0)
    U_train = torch.cat(all_logits, dim=0)
    Y_train = torch.cat(all_labels, dim=0)
    n_train = F_train.shape[0]
    print(f"Training set size: {n_train}, Feature dimension: {F_train.shape[1]}")

    # --- æ­¥éª¤ 2: è®¡ç®—å†…åœ¨Shapleyå€¼çŸ©é˜µ Î¦ ---
    print("--- Step 2/4: Calculating Intrinsic Shapley Value Matrix (Î¦)...")
    Gamma = (torch.nn.functional.one_hot(Y_train, num_classes=num_classes) - torch.softmax(U_train, dim=-1)) / lambda_reg
    mu_list, J_mu_list = [None] * num_classes, [None] * num_classes
    psi = lambda x: torch.softmax(x, dim=-1)
    for c in tqdm(range(num_classes), desc="Calculating Jacobians per class"):
        mu_c = U_train[Y_train == c].mean(dim=0)
        J_mu_c = torch.func.jacrev(psi)(mu_c)
        mu_list[c], J_mu_list[c] = mu_c, J_mu_c
    Phi = torch.zeros(n_train, num_classes)
    for c in tqdm(range(num_classes), desc="Calculating Î¦ matrix"):
        grad_c = J_mu_list[c][c, :]
        phi_for_class_c = Gamma @ grad_c
        Phi[:, c] = phi_for_class_c
    print("Intrinsic Shapley Value Matrix (Î¦) calculated.")

    # --- æ­¥éª¤ 3: èšåˆæµ‹è¯•é›†ä¿¡æ¯ (ä¼˜åŒ–ç‰ˆæœ¬) ---
    print("--- Step 3/4: Aggregating similarities over the test set (optimized)...")
    if num_test_samples_to_use == -1 or num_test_samples_to_use > len(test_loader.dataset):
        num_test_samples_to_use = len(test_loader.dataset)
    F_train_gpu = F_train.to(device)
    accumulated_kernel_sim = torch.zeros(n_train, device=device)
    test_samples_processed = 0
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªbatchå‡å°‘çŸ©é˜µä¹˜æ³•æ¬¡æ•°
    batch_buffer = []
    BUFFER_SIZE = 16  # å•GPUå¯ä»¥ç”¨æ›´å¤§çš„buffer
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(test_loader, desc="Aggregating test similarities")):
            if test_samples_processed >= num_test_samples_to_use: break
            
            if model_type == 'transformer':
                # å¤„ç†æ–‡æœ¬æ•°æ®
                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                batch_size = inputs['input_ids'].size(0)
            else:
                # å¤„ç†å›¾åƒæ•°æ®
                inputs, _ = batch
                inputs = inputs.to(device)
                batch_size = inputs.size(0)

            # è®¡ç®—å½“å‰batchçš„ç‰¹å¾
            _, features_batch = wrapper(inputs)
            batch_buffer.append(features_batch)
            test_samples_processed += batch_size
            
            # å½“bufferæ»¡äº†æˆ–è€…æ˜¯æœ€åä¸€ä¸ªbatchæ—¶ï¼Œè¿›è¡Œå¤„ç†
            if len(batch_buffer) >= BUFFER_SIZE or batch_idx == len(test_loader) - 1 or test_samples_processed >= num_test_samples_to_use:
                # åˆå¹¶bufferä¸­çš„ç‰¹å¾
                if len(batch_buffer) > 1:
                    combined_features = torch.cat(batch_buffer, dim=0)
                else:
                    combined_features = batch_buffer[0]
                
                # åˆ†å—è®¡ç®—ä»¥é¿å…å†…å­˜æº¢å‡º
                chunk_size = min(2000, combined_features.size(0))  # å•GPUå¯ä»¥å¤„ç†æ›´å¤§çš„chunk
                for i in range(0, combined_features.size(0), chunk_size):
                    chunk_features = combined_features[i:i+chunk_size]
                    kernel_chunk = torch.mm(chunk_features, F_train_gpu.T)
                    accumulated_kernel_sim += kernel_chunk.sum(dim=0)
                
                # æ¸…ç©ºbuffer
                batch_buffer.clear()
    
    avg_kernel_sim = (accumulated_kernel_sim / (test_samples_processed + 1e-8)).cpu()

    # --- æ­¥éª¤ 4: è®¡ç®—æœ€ç»ˆçš„ç»¼åˆShapleyå€¼å‘é‡ ---
    print("--- Step 4/4: Calculating final aggregated Shapley vectors...")
    aggregated_shapley_vectors = Phi * avg_kernel_sim.unsqueeze(1)
    
    print("--- Single GPU Shapley vectors calculation is complete ---")
    return aggregated_shapley_vectors, Y_train

def calculate_shapley_vectors_multi_gpu(model, train_loader, test_loader, num_classes, lambda_reg, num_test_samples_to_use=-1, model_type='resnet'):
    """
    ä½¿ç”¨Accelerateæ¡†æ¶çš„å¤šGPUåŠ é€ŸShapleyå€¼è®¡ç®—å‡½æ•°
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        num_classes: ç±»åˆ«æ•°é‡
        lambda_reg: æ­£åˆ™åŒ–å‚æ•°
        num_test_samples_to_use: ä½¿ç”¨çš„æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆ-1è¡¨ç¤ºå…¨éƒ¨ï¼‰
        model_type: æ¨¡å‹ç±»å‹ ('resnet' æˆ– 'transformer')
    
    Returns:
        tuple: (shapley_vectors, train_labels) æˆ– (None, None) å¦‚æœä¸æ˜¯ä¸»è¿›ç¨‹
    """
    # åˆå§‹åŒ–Accelerator
    accelerator = Accelerator()
    device = accelerator.device
    
    if accelerator.is_main_process:
        print("ğŸš€ ä½¿ç”¨AccelerateåŠ é€ŸShapleyå€¼è®¡ç®—")
        print(f"è®¾å¤‡æ•°é‡: {accelerator.num_processes}")
        print(f"å½“å‰è®¾å¤‡: {device}")
    
    # ä½¿ç”¨ModelWrapperå¹¶prepare
    wrapper = ModelWrapper(model, model_type)
    wrapper.eval()
    
    # Prepareæ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
    wrapper, train_loader, test_loader = accelerator.prepare(wrapper, train_loader, test_loader)
    
    # --- æ­¥éª¤ 1: å¹¶è¡Œæå–ç‰¹å¾å’Œlogits ---
    if accelerator.is_main_process:
        print("--- Step 1/4: Extracting features and logits (accelerated)...")
    
    all_features, all_logits, all_labels = [], [], []
    
    with torch.no_grad():
        for batch in tqdm(train_loader, desc="Extracting train data", disable=not accelerator.is_main_process):
            if model_type == 'transformer':
                # å¤„ç†æ–‡æœ¬æ•°æ®
                inputs = {k: v for k, v in batch.items() if k != 'labels'}
                labels = batch['labels']
            else:
                # å¤„ç†å›¾åƒæ•°æ®
                inputs, labels = batch
            
            # ä½¿ç”¨acceleratorè‡ªåŠ¨å¤„ç†è®¾å¤‡åˆ†é…
            logits, features = wrapper(inputs)
            
            # æ”¶é›†æ‰€æœ‰GPUçš„ç»“æœ
            features = accelerator.gather(features)
            logits = accelerator.gather(logits)
            labels = accelerator.gather(labels)
            
            if accelerator.is_main_process:
                all_features.append(features.cpu())
                all_logits.append(logits.cpu())
                all_labels.append(labels.cpu())
    
    # ç¡®ä¿åªåœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œåç»­è®¡ç®—
    if not accelerator.is_main_process:
        return None, None
        
    F_train = torch.cat(all_features, dim=0)
    U_train = torch.cat(all_logits, dim=0)
    Y_train = torch.cat(all_labels, dim=0)
    n_train = F_train.shape[0]
    print(f"Training set size: {n_train}, Feature dimension: {F_train.shape[1]}")

    # --- æ­¥éª¤ 2: è®¡ç®—å†…åœ¨Shapleyå€¼çŸ©é˜µ Î¦ ---
    print("--- Step 2/4: Calculating Intrinsic Shapley Value Matrix (Î¦)...")
    # å°†è®¡ç®—ç§»åˆ°GPUä¸ŠåŠ é€Ÿ
    Gamma = (torch.nn.functional.one_hot(Y_train, num_classes=num_classes).to(device) - 
             torch.softmax(U_train.to(device), dim=-1)) / lambda_reg
    
    mu_list, J_mu_list = [None] * num_classes, [None] * num_classes
    psi = lambda x: torch.softmax(x, dim=-1)
    
    # å¹¶è¡Œè®¡ç®—æ¯ä¸ªç±»åˆ«çš„Jacobian
    U_train_gpu = U_train.to(device)
    Y_train_gpu = Y_train.to(device)
    
    for c in tqdm(range(num_classes), desc="Calculating Jacobians per class"):
        mu_c = U_train_gpu[Y_train_gpu == c].mean(dim=0)
        J_mu_c = torch.func.jacrev(psi)(mu_c)
        mu_list[c], J_mu_list[c] = mu_c, J_mu_c
    
    # ä½¿ç”¨GPUå¹¶è¡Œè®¡ç®—PhiçŸ©é˜µ
    Phi = torch.zeros(n_train, num_classes, device=device)
    for c in tqdm(range(num_classes), desc="Calculating Î¦ matrix"):
        grad_c = J_mu_list[c][c, :]
        phi_for_class_c = Gamma @ grad_c
        Phi[:, c] = phi_for_class_c
    print("Intrinsic Shapley Value Matrix (Î¦) calculated.")

    # --- æ­¥éª¤ 3: å¹¶è¡Œèšåˆæµ‹è¯•é›†ä¿¡æ¯ (ä¼˜åŒ–ç‰ˆæœ¬) ---
    print("--- Step 3/4: Aggregating similarities over the test set (accelerated & optimized)...")
    if num_test_samples_to_use == -1 or num_test_samples_to_use > len(test_loader.dataset):
        num_test_samples_to_use = len(test_loader.dataset)
    
    F_train_gpu = F_train.to(device)
    # ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡æ¥å‡å°‘gatheræ“ä½œæ¬¡æ•°
    accumulated_kernel_sim = torch.zeros(n_train, device=device)
    test_samples_processed = 0
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªbatchå‡å°‘é€šä¿¡å¼€é”€
    batch_buffer = []
    BUFFER_SIZE = 8  # ç´¯ç§¯8ä¸ªbatchåå†åšä¸€æ¬¡gather
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(test_loader, desc="Aggregating test similarities", disable=not accelerator.is_main_process)):
            if test_samples_processed >= num_test_samples_to_use: 
                break
            
            if model_type == 'transformer':
                inputs = {k: v for k, v in batch.items() if k != 'labels'}
                current_batch_size = inputs['input_ids'].size(0)
            else:
                inputs, _ = batch
                current_batch_size = inputs.size(0)

            # è®¡ç®—å½“å‰batchçš„ç‰¹å¾
            _, features_batch = wrapper(inputs)
            batch_buffer.append(features_batch)
            test_samples_processed += current_batch_size
            
            # å½“bufferæ»¡äº†æˆ–è€…æ˜¯æœ€åä¸€ä¸ªbatchæ—¶ï¼Œè¿›è¡Œå¤„ç†
            if len(batch_buffer) >= BUFFER_SIZE or batch_idx == len(test_loader) - 1 or test_samples_processed >= num_test_samples_to_use:
                # åˆå¹¶bufferä¸­çš„ç‰¹å¾
                if len(batch_buffer) > 1:
                    combined_features = torch.cat(batch_buffer, dim=0)
                else:
                    combined_features = batch_buffer[0]
                
                # å¹¶è¡Œgatheræ‰€æœ‰GPUçš„ç»“æœ
                combined_features = accelerator.gather(combined_features)
                
                if accelerator.is_main_process:
                    # ä½¿ç”¨æ›´é«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•
                    # åˆ†å—è®¡ç®—ä»¥é¿å…å†…å­˜æº¢å‡º
                    chunk_size = min(1000, combined_features.size(0))
                    for i in range(0, combined_features.size(0), chunk_size):
                        chunk_features = combined_features[i:i+chunk_size]
                        kernel_chunk = torch.mm(chunk_features, F_train_gpu.T)
                        accumulated_kernel_sim += kernel_chunk.sum(dim=0)
                
                # æ¸…ç©ºbuffer
                batch_buffer.clear()
    
    if accelerator.is_main_process:
        # è®¡ç®—æ‰€æœ‰GPUçš„æ€»æ ·æœ¬æ•°
        total_samples_processed = test_samples_processed * accelerator.num_processes
        # ä½†ä¸è¦è¶…è¿‡å®é™…è¦å¤„ç†çš„æ ·æœ¬æ•°
        total_samples_processed = min(total_samples_processed, num_test_samples_to_use)
        avg_kernel_sim = (accumulated_kernel_sim / (total_samples_processed + 1e-8))
    else:
        avg_kernel_sim = None

    # --- æ­¥éª¤ 4: è®¡ç®—æœ€ç»ˆShapleyå€¼ ---
    if accelerator.is_main_process:
        print("--- Step 4/4: Calculating final aggregated Shapley vectors...")
        aggregated_shapley_vectors = Phi * avg_kernel_sim.unsqueeze(1)
        
        # ç§»å›CPUä»¥èŠ‚çœGPUå†…å­˜
        aggregated_shapley_vectors = aggregated_shapley_vectors.cpu()
        Y_train = Y_train.cpu()
        
        print("--- Accelerated Shapley vectors calculation is complete ---")
        print(f"âš¡ åŠ é€Ÿè®¡ç®—å®Œæˆï¼Œä½¿ç”¨äº† {accelerator.num_processes} ä¸ªGPU")
        
        return aggregated_shapley_vectors, Y_train
    else:
        return None, None

# ================================
# ä¾¿æ·ä½¿ç”¨æ¥å£
# ================================

def load_data_and_model(model_type='resnet', config=None):
    """
    ç»Ÿä¸€çš„æ•°æ®å’Œæ¨¡å‹åŠ è½½å‡½æ•°
    
    Args:
        model_type: 'resnet' æˆ– 'transformer'
        config: é…ç½®å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
    
    Returns:
        tuple: (train_loader, test_loader, model, num_classes, original_labels, flipped_indices)
    """
    if config is None:
        if model_type == 'transformer':
            config = settings.TRANSFORMER_SHAPLEY_CONFIG
        else:
            config = settings.RESNET_SHAPLEY_CONFIG
    
    print(f"=== Shapleyå€¼è®¡ç®—ï¼š{model_type.upper()}æ¨¡å‹ ===")
    print(f"æ¨¡å‹: {config['model_name']}")
    if model_type == 'transformer':
        print(f"æ•°æ®é›†: {config['dataset_name']}")
    else:
        print(f"æ•°æ®é›†: {settings.RESNET_DATASET}")
    print()
    
    # è®¾ç½®éšæœºç§å­
    utils.set_seed(settings.SEED)
    
    # åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    if model_type == 'transformer':
        train_loader, test_loader, _, _, num_classes, original_labels, flipped_indices = utils.get_text_data_loaders(
            dataset_name=config['dataset_name'],
            tokenizer_name=config['model_name'],
            batch_size=settings.BATCH_SIZE,
            label_noise_rate=config.get('label_noise_rate', 0.0)
        )
    else:
        train_loader, test_loader, _, _, num_classes, original_labels, flipped_indices = utils.get_data_loaders(
            dataset_name=settings.RESNET_DATASET,
            batch_size=settings.BATCH_SIZE,
            shuffle_train=False,
            label_noise_rate=config.get('label_noise_rate', 0.0)
        )
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ¤– åŠ è½½æ¨¡å‹...")
    if model_type == 'transformer':
        checkpoint_path = None
        if 'checkpoint_name' in config:
            checkpoint_path = os.path.join(settings.CHECKPOINT_DIR, config['checkpoint_name'])
            if not os.path.exists(checkpoint_path):
                print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°checkpointæ–‡ä»¶ {checkpoint_path}")
                print("å°†ä½¿ç”¨æœªå¾®è°ƒçš„æ¨¡å‹")
                checkpoint_path = None
            else:
                print(f"âœ… ä½¿ç”¨checkpoint: {checkpoint_path}")
        
        model = utils.get_transformer_model(
            model_name=config['model_name'],
            num_classes=num_classes,
            checkpoint_path=checkpoint_path,
            use_bf16=config.get('use_bf16', False)
        )
    else:
        model = utils.get_model(
            model_name=config['model_name'],
            num_classes=num_classes,
            from_scratch=True,
            checkpoint_path=os.path.join(settings.CHECKPOINT_DIR, config['checkpoint_name'])
        )
    
    return train_loader, test_loader, model, num_classes, original_labels, flipped_indices, config

def calculate_shapley_unified(model_type='resnet', use_accelerate=False, config=None):
    """
    ç»Ÿä¸€çš„Shapleyå€¼è®¡ç®—æ¥å£
    
    Args:
        model_type: 'resnet' æˆ– 'transformer'
        use_accelerate: æ˜¯å¦ä½¿ç”¨å¤šGPUåŠ é€Ÿ
        config: é…ç½®å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
    
    Returns:
        tuple: (shapley_vectors, train_labels, save_path)
    """
    # åŠ è½½æ•°æ®å’Œæ¨¡å‹
    train_loader, test_loader, model, num_classes, original_labels, flipped_indices, config = load_data_and_model(
        model_type, config
    )
    
    # é€‰æ‹©è®¡ç®—å‡½æ•°å’Œè®¾å¤‡é…ç½®
    if use_accelerate:
        print("ğŸš€ ä½¿ç”¨å¤šGPUåŠ é€Ÿæ¨¡å¼")
        
        # è®¡ç®—Shapleyå€¼
        shapley_vectors, train_labels = calculate_shapley_vectors_multi_gpu(
            model=model,
            train_loader=train_loader,
            test_loader=test_loader,
            num_classes=num_classes,
            lambda_reg=config['lambda_reg'],
            num_test_samples_to_use=config['num_test_samples_to_use'],
            model_type=model_type
        )
    else:
        print("ğŸ’» ä½¿ç”¨å•GPUæ¨¡å¼")
        device = torch.device(settings.DEVICE if torch.cuda.is_available() else "cpu")
        print(f"è®¾å¤‡: {device}")
        model = model.to(device)
        
        # è®¡ç®—Shapleyå€¼
        shapley_vectors, train_labels = calculate_shapley_vectors_single_gpu(
            model=model,
            train_loader=train_loader,
            test_loader=test_loader,
            num_classes=num_classes,
            device=device,
            lambda_reg=config['lambda_reg'],
            num_test_samples_to_use=config['num_test_samples_to_use'],
            model_type=model_type
        )
    
    # å¦‚æœæ˜¯å¤šGPUæ¨¡å¼ä¸”ä¸æ˜¯ä¸»è¿›ç¨‹ï¼Œè¿”å›None
    if use_accelerate and shapley_vectors is None:
        return None, None, None
    
    # ä¿å­˜ç»“æœ
    save_path = save_shapley_results(
        shapley_vectors, train_labels, original_labels, flipped_indices, 
        config, model_type
    )
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print_shapley_statistics(shapley_vectors, train_labels, flipped_indices, save_path)
    
    return shapley_vectors, train_labels, save_path

def save_shapley_results(shapley_vectors, train_labels, original_labels, flipped_indices, config, model_type):
    """ä¿å­˜Shapleyè®¡ç®—ç»“æœ"""
    save_dir = settings.RESULTS_DIR
    os.makedirs(save_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    if model_type == 'transformer':
        model_name_safe = config['model_name'].replace('/', '_').replace('-', '_')
        dataset_name = config['dataset_name']
    else:
        model_name_safe = config['model_name']
        dataset_name = settings.RESNET_DATASET
    
    save_path = os.path.join(
        save_dir,
        f"shapley_vectors_{dataset_name}_{model_name_safe}_noise_{config.get('label_noise_rate', 0.0)}.pt"
    )
    
    # ä¿å­˜æ•°æ®
    torch.save({
        'shapley_vectors': shapley_vectors,
        'noisy_labels': train_labels,  # ä¿æŒä¸åŸå§‹ä»£ç çš„ä¸€è‡´æ€§
        'train_labels': train_labels,  # æ–°å¢çš„é”®å
        'original_labels': torch.tensor(original_labels),
        'flipped_indices': torch.tensor(flipped_indices),
        'config': config
    }, save_path)
    
    return save_path

def print_shapley_statistics(shapley_vectors, train_labels, flipped_indices, save_path):
    """æ‰“å°Shapleyå€¼ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ‰ è®¡ç®—å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
    print(f"ğŸ“Š Shapleyå‘é‡å½¢çŠ¶: {shapley_vectors.shape}")
    print(f"ğŸ“ è®­ç»ƒæ ·æœ¬æ•°é‡: {len(train_labels)}")
    
    # åŸºæœ¬ç»Ÿè®¡
    print("\n=== ğŸ“ˆ ç»Ÿè®¡åˆ†æ ===")
    shapley_norms = torch.linalg.norm(shapley_vectors, dim=1)
    print(f"å¹³å‡å€¼: {shapley_norms.mean():.6f}")
    print(f"æ ‡å‡†å·®: {shapley_norms.std():.6f}")
    print(f"æœ€å¤§å€¼: {shapley_norms.max():.6f}")
    print(f"æœ€å°å€¼: {shapley_norms.min():.6f}")
    
    # æ ‡ç­¾å™ªå£°ç»Ÿè®¡
    if len(flipped_indices) > 0:
        print(f"\n=== ğŸ·ï¸ æ ‡ç­¾å™ªå£°ç»Ÿè®¡ ===")
        print(f"ç¿»è½¬æ ·æœ¬æ•°é‡: {len(flipped_indices)}")
        print(f"ç¿»è½¬æ¯”ä¾‹: {len(flipped_indices)/len(train_labels)*100:.2f}%")

def run_shapley_calculation(model_type='resnet', use_accelerate=False):
    """
    ä¾¿æ·çš„è¿è¡Œå‡½æ•°
    
    Args:
        model_type: 'resnet' æˆ– 'transformer'
        use_accelerate: æ˜¯å¦ä½¿ç”¨å¤šGPUåŠ é€Ÿ
    """
    try:
        shapley_vectors, train_labels, save_path = calculate_shapley_unified(
            model_type=model_type,
            use_accelerate=use_accelerate
        )
        
        if shapley_vectors is not None:  # ç¡®ä¿ä¸æ˜¯å¤šGPUçš„éä¸»è¿›ç¨‹
            print(f"\nâœ… {model_type.upper()} Shapleyå€¼è®¡ç®—æˆåŠŸå®Œæˆï¼")
            if use_accelerate:
                print("âš¡ å¤šGPUåŠ é€Ÿæ˜¾è‘—æå‡äº†è®¡ç®—é€Ÿåº¦")
        
        return shapley_vectors, train_labels, save_path
        
    except Exception as e:
        print(f"âŒ è®¡ç®—è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€çš„Shapleyå€¼è®¡ç®—å·¥å…·")
    parser.add_argument("--model-type", choices=["resnet", "transformer"], 
                       default="resnet", help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--accelerate", action="store_true", 
                       help="ä½¿ç”¨å¤šGPUåŠ é€Ÿ")
    
    args = parser.parse_args()
    
    if args.accelerate:
        print("ğŸš€ å¯ç”¨å¤šGPUåŠ é€Ÿæ¨¡å¼")
        print("è¯·ç¡®ä¿ä½¿ç”¨ 'accelerate launch utils/shapley_utils.py --accelerate' è¿è¡Œ")
    
    run_shapley_calculation(args.model_type, args.accelerate)