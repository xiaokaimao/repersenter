#!/usr/bin/env python3
"""
Shapleyå€¼è´¡çŒ®åˆ†æå’Œé˜ˆå€¼ä¼˜åŒ–è„šæœ¬
åˆ†æç¿»è½¬æ ·æœ¬å’Œæ­£ç¡®æ ·æœ¬çš„è´¡çŒ®å€¼åˆ†å¸ƒï¼Œæ‰¾å‡ºæœ€ä½³æ£€æµ‹é˜ˆå€¼
"""
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
import config.settings as settings

def load_shapley_data(experiment_type='resnet'):
    """åŠ è½½Shapleyå€¼æ•°æ®"""
    config = settings.CORE_SET_EXPERIMENT_CONFIG[experiment_type]
    
    if experiment_type == 'transformer':
        model_name_safe = config['model_name'].replace('/', '_')
        shapley_file_name = f"shapley_vectors_{config['dataset_name']}_{model_name_safe}_noise_{config.get('label_noise_rate', 0.0)}.pt"
    else:
        shapley_file_name = f"shapley_vectors_{config['dataset_name']}_{config['model_name']}_noise_{config.get('label_noise_rate', 0.0)}.pt"
    
    load_path = os.path.join(settings.RESULTS_DIR, shapley_file_name)
    
    if not os.path.exists(load_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°Shapleyæ–‡ä»¶: {load_path}")
    
    print(f"åŠ è½½Shapleyæ•°æ®: {load_path}")
    return torch.load(load_path, map_location='cpu', weights_only=False)

def calculate_original_label_contribution(shapley_vectors, original_labels):
    """
    è®¡ç®—æ¯ä¸ªæ ·æœ¬å¯¹å…¶åŸå§‹çœŸå®æ ‡ç­¾çš„è´¡çŒ®
    
    Args:
        shapley_vectors: Shapleyå€¼çŸ©é˜µ (n_samples, n_classes)
        original_labels: åŸå§‹çœŸå®æ ‡ç­¾
    
    Returns:
        numpy.array: æ¯ä¸ªæ ·æœ¬å¯¹å…¶åŸå§‹æ ‡ç­¾çš„è´¡çŒ®åˆ†æ•°
    """
    # æå–æ¯ä¸ªæ ·æœ¬å¯¹å…¶åŸå§‹çœŸå®æ ‡ç­¾çš„Shapleyå€¼
    indices = torch.arange(len(original_labels))
    contribution_scores = shapley_vectors[indices, original_labels].numpy()
    return contribution_scores

def analyze_contribution_distribution(shapley_data):
    """åˆ†æè´¡çŒ®å€¼åˆ†å¸ƒ"""
    shapley_vectors = shapley_data['shapley_vectors']
    original_labels = shapley_data.get('original_labels', shapley_data['noisy_labels'])
    flipped_indices = shapley_data.get('flipped_indices', [])
    
    # è®¡ç®—å¯¹åŸå§‹æ ‡ç­¾çš„è´¡çŒ®
    contribution_scores = calculate_original_label_contribution(shapley_vectors, original_labels)
    
    # åˆ†ç»„ç´¢å¼•
    all_indices = np.arange(len(original_labels))
    correct_indices = np.setdiff1d(all_indices, flipped_indices)
    
    # æå–è´¡çŒ®åˆ†æ•°
    if len(flipped_indices) > 0:
        flipped_scores = contribution_scores[flipped_indices]
        correct_scores = contribution_scores[correct_indices]
    else:
        flipped_scores = np.array([])
        correct_scores = contribution_scores
    
    return {
        'all_scores': contribution_scores,
        'flipped_scores': flipped_scores,
        'correct_scores': correct_scores,
        'flipped_indices': flipped_indices,
        'correct_indices': correct_indices
    }

def find_optimal_threshold(contribution_data):
    """æ‰¾å‡ºä½¿F1åˆ†æ•°æœ€å¤§åŒ–çš„æœ€ä½³é˜ˆå€¼"""
    if len(contribution_data['flipped_scores']) == 0:
        print("è­¦å‘Š: æ²¡æœ‰ç¿»è½¬æ ·æœ¬ï¼Œæ— æ³•è®¡ç®—æœ€ä½³é˜ˆå€¼")
        return None, None, None
    
    # åˆ›å»ºçœŸå®æ ‡ç­¾ (1è¡¨ç¤ºç¿»è½¬æ ·æœ¬ï¼Œ0è¡¨ç¤ºæ­£ç¡®æ ·æœ¬)
    y_true = np.concatenate([
        np.ones(len(contribution_data['flipped_scores'])),  # ç¿»è½¬æ ·æœ¬
        np.zeros(len(contribution_data['correct_scores']))  # æ­£ç¡®æ ·æœ¬
    ])
    
    # è´¡çŒ®åˆ†æ•° (ç”¨è´Ÿå€¼ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›ä½è´¡çŒ®å€¼è¡¨ç¤ºç¿»è½¬æ ·æœ¬)
    scores = np.concatenate([
        contribution_data['flipped_scores'],
        contribution_data['correct_scores']
    ])
    
    # ä½¿ç”¨è´Ÿåˆ†æ•°è¿›è¡Œprecision-recallæ›²çº¿è®¡ç®—
    # å› ä¸ºæˆ‘ä»¬å¸Œæœ›ä½è´¡çŒ®å€¼å¯¹åº”é«˜"å¼‚å¸¸"åˆ†æ•°
    precision, recall, thresholds = precision_recall_curve(y_true, -scores)
    
    # è®¡ç®—F1åˆ†æ•°ï¼ˆé¿å…é™¤é›¶é”™è¯¯ï¼‰
    f1_scores = 2 * precision * recall / (precision + recall + 1e-8)
    
    # æ‰¾å‡ºæœ€ä½³é˜ˆå€¼
    best_idx = np.argmax(f1_scores)
    best_threshold = -thresholds[best_idx]  # è½¬æ¢å›åŸå§‹é˜ˆå€¼
    best_f1 = f1_scores[best_idx]
    best_precision = precision[best_idx]
    best_recall = recall[best_idx]
    
    # ç¡®ä¿æ•°ç»„é•¿åº¦ä¸€è‡´
    min_length = min(len(thresholds), len(f1_scores), len(precision), len(recall))
    
    return best_threshold, best_f1, {
        'precision': best_precision,
        'recall': best_recall,
        'f1_score': best_f1,
        'all_thresholds': -thresholds[:min_length],
        'all_f1_scores': f1_scores[:min_length],
        'all_precision': precision[:min_length],
        'all_recall': recall[:min_length]
    }

def evaluate_threshold(contribution_data, threshold):
    """è¯„ä¼°ç‰¹å®šé˜ˆå€¼çš„æ€§èƒ½"""
    if len(contribution_data['flipped_scores']) == 0:
        return None
    
    # é¢„æµ‹ (è´¡çŒ®å€¼ < é˜ˆå€¼ çš„æ ·æœ¬è¢«é¢„æµ‹ä¸ºç¿»è½¬æ ·æœ¬)
    flipped_predictions = contribution_data['flipped_scores'] < threshold
    correct_predictions = contribution_data['correct_scores'] < threshold
    
    # ç»Ÿè®¡
    tp = np.sum(flipped_predictions)  # ç¿»è½¬æ ·æœ¬ä¸­è¢«æ­£ç¡®æ£€æµ‹çš„
    fp = np.sum(correct_predictions)  # æ­£ç¡®æ ·æœ¬ä¸­è¢«é”™è¯¯æ£€æµ‹çš„
    fn = np.sum(~flipped_predictions)  # ç¿»è½¬æ ·æœ¬ä¸­è¢«é—æ¼çš„
    tn = np.sum(~correct_predictions)  # æ­£ç¡®æ ·æœ¬ä¸­è¢«æ­£ç¡®è¯†åˆ«çš„
    
    # è®¡ç®—æŒ‡æ ‡
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    return {
        'threshold': threshold,
        'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'num_detected': tp + fp,
        'detection_rate': (tp + fp) / len(contribution_data['all_scores'])
    }

def plot_analysis_results(contribution_data, optimal_results, save_path=None):
    """ç»˜åˆ¶åˆ†æç»“æœ"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. è´¡çŒ®å€¼åˆ†å¸ƒç›´æ–¹å›¾
    ax1 = axes[0, 0]
    if len(contribution_data['flipped_scores']) > 0:
        ax1.hist(contribution_data['correct_scores'], bins=50, density=True, alpha=0.7, 
                label=f'æ­£ç¡®æ ·æœ¬ (n={len(contribution_data["correct_scores"])})', color='green')
        ax1.hist(contribution_data['flipped_scores'], bins=30, density=True, alpha=0.8, 
                label=f'ç¿»è½¬æ ·æœ¬ (n={len(contribution_data["flipped_scores"])})', color='red')
    else:
        ax1.hist(contribution_data['correct_scores'], bins=50, density=True, alpha=0.7, 
                label=f'æ‰€æœ‰æ ·æœ¬ (n={len(contribution_data["correct_scores"])})', color='blue')
    
    ax1.axvline(x=0, color='black', linestyle='--', linewidth=2, label='é˜ˆå€¼=0')
    if optimal_results and 'best_threshold' in optimal_results:
        ax1.axvline(x=optimal_results['best_threshold'], color='purple', linestyle='--', 
                   linewidth=2, label=f'æœ€ä½³é˜ˆå€¼={optimal_results["best_threshold"]:.3f}')
    
    ax1.set_title('è´¡çŒ®å€¼åˆ†å¸ƒ')
    ax1.set_xlabel('å¯¹åŸå§‹æ ‡ç­¾çš„è´¡çŒ®å€¼')
    ax1.set_ylabel('å¯†åº¦')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. è´¡çŒ®å€¼ç®±å‹å›¾
    ax2 = axes[0, 1]
    if len(contribution_data['flipped_scores']) > 0:
        data_for_box = [contribution_data['correct_scores'], contribution_data['flipped_scores']]
        labels_for_box = ['æ­£ç¡®æ ·æœ¬', 'ç¿»è½¬æ ·æœ¬']
        colors = ['green', 'red']
    else:
        data_for_box = [contribution_data['correct_scores']]
        labels_for_box = ['æ‰€æœ‰æ ·æœ¬']
        colors = ['blue']
    
    box_plot = ax2.boxplot(data_for_box, tick_labels=labels_for_box, patch_artist=True)
    for patch, color in zip(box_plot['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    ax2.axhline(y=0, color='black', linestyle='--', linewidth=2, label='é˜ˆå€¼=0')
    if optimal_results and 'best_threshold' in optimal_results:
        ax2.axhline(y=optimal_results['best_threshold'], color='purple', linestyle='--', 
                   linewidth=2, label=f'æœ€ä½³é˜ˆå€¼={optimal_results["best_threshold"]:.3f}')
    
    ax2.set_title('è´¡çŒ®å€¼ç®±å‹å›¾')
    ax2.set_ylabel('å¯¹åŸå§‹æ ‡ç­¾çš„è´¡çŒ®å€¼')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. F1åˆ†æ•°éšé˜ˆå€¼å˜åŒ–
    ax3 = axes[1, 0]
    if optimal_results and 'metrics' in optimal_results:
        metrics = optimal_results['metrics']
        ax3.plot(metrics['all_thresholds'], metrics['all_f1_scores'], 'b-', linewidth=2, label='F1åˆ†æ•°')
        ax3.plot(metrics['all_thresholds'], metrics['all_precision'], 'g--', linewidth=2, label='ç²¾ç¡®ç‡')
        ax3.plot(metrics['all_thresholds'], metrics['all_recall'], 'r--', linewidth=2, label='å¬å›ç‡')
        
        # æ ‡è®°æœ€ä½³ç‚¹
        ax3.scatter([optimal_results['best_threshold']], [optimal_results['best_f1']], 
                   color='purple', s=100, zorder=5, label=f'æœ€ä½³ç‚¹ (F1={optimal_results["best_f1"]:.3f})')
    
    ax3.set_title('æ€§èƒ½æŒ‡æ ‡éšé˜ˆå€¼å˜åŒ–')
    ax3.set_xlabel('é˜ˆå€¼')
    ax3.set_ylabel('åˆ†æ•°')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. æ£€æµ‹ç»Ÿè®¡
    ax4 = axes[1, 1]
    if len(contribution_data['flipped_scores']) > 0:
        # è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ç»Ÿè®¡
        thresholds_to_test = [0.0]
        if optimal_results and 'best_threshold' in optimal_results:
            thresholds_to_test.append(optimal_results['best_threshold'])
        
        results_data = []
        for thresh in thresholds_to_test:
            eval_result = evaluate_threshold(contribution_data, thresh)
            if eval_result:
                results_data.append(eval_result)
        
        if results_data:
            thresh_labels = [f'é˜ˆå€¼={r["threshold"]:.3f}' for r in results_data]
            f1_scores = [r['f1_score'] for r in results_data]
            precision_scores = [r['precision'] for r in results_data]
            recall_scores = [r['recall'] for r in results_data]
            
            x = np.arange(len(thresh_labels))
            width = 0.25
            
            ax4.bar(x - width, f1_scores, width, label='F1åˆ†æ•°', alpha=0.8)
            ax4.bar(x, precision_scores, width, label='ç²¾ç¡®ç‡', alpha=0.8)
            ax4.bar(x + width, recall_scores, width, label='å¬å›ç‡', alpha=0.8)
            
            ax4.set_xlabel('é˜ˆå€¼')
            ax4.set_ylabel('åˆ†æ•°')
            ax4.set_title('ä¸åŒé˜ˆå€¼ä¸‹çš„æ€§èƒ½æ¯”è¾ƒ')
            ax4.set_xticks(x)
            ax4.set_xticklabels(thresh_labels)
            ax4.legend()
            ax4.grid(True, alpha=0.3)
    else:
        ax4.text(0.5, 0.5, 'æ²¡æœ‰ç¿»è½¬æ ·æœ¬\næ— æ³•è®¡ç®—æ€§èƒ½æŒ‡æ ‡', 
                ha='center', va='center', transform=ax4.transAxes, fontsize=14)
        ax4.set_title('æ€§èƒ½æŒ‡æ ‡')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()

def print_detailed_statistics(contribution_data, optimal_results):
    """æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "="*60)
    print("ğŸ“Š è¯¦ç»†ç»Ÿè®¡åˆ†æ")
    print("="*60)
    
    # åŸºæœ¬ç»Ÿè®¡
    print("\n1. åŸºæœ¬ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(contribution_data['all_scores'])}")
    print(f"   ç¿»è½¬æ ·æœ¬æ•°: {len(contribution_data['flipped_scores'])}")
    print(f"   æ­£ç¡®æ ·æœ¬æ•°: {len(contribution_data['correct_scores'])}")
    if len(contribution_data['flipped_scores']) > 0:
        print(f"   ç¿»è½¬æ¯”ä¾‹: {len(contribution_data['flipped_scores']) / len(contribution_data['all_scores']) * 100:.2f}%")
    
    # è´¡çŒ®å€¼ç»Ÿè®¡
    print("\n2. è´¡çŒ®å€¼ç»Ÿè®¡:")
    all_scores = contribution_data['all_scores']
    print(f"   æ•´ä½“å¹³å‡å€¼: {np.mean(all_scores):.6f}")
    print(f"   æ•´ä½“æ ‡å‡†å·®: {np.std(all_scores):.6f}")
    print(f"   æ•´ä½“æœ€å°å€¼: {np.min(all_scores):.6f}")
    print(f"   æ•´ä½“æœ€å¤§å€¼: {np.max(all_scores):.6f}")
    
    if len(contribution_data['flipped_scores']) > 0:
        flipped_scores = contribution_data['flipped_scores']
        correct_scores = contribution_data['correct_scores']
        
        print(f"\n   ç¿»è½¬æ ·æœ¬è´¡çŒ®å€¼:")
        print(f"     å¹³å‡å€¼: {np.mean(flipped_scores):.6f}")
        print(f"     æ ‡å‡†å·®: {np.std(flipped_scores):.6f}")
        print(f"     æœ€å°å€¼: {np.min(flipped_scores):.6f}")
        print(f"     æœ€å¤§å€¼: {np.max(flipped_scores):.6f}")
        
        print(f"\n   æ­£ç¡®æ ·æœ¬è´¡çŒ®å€¼:")
        print(f"     å¹³å‡å€¼: {np.mean(correct_scores):.6f}")
        print(f"     æ ‡å‡†å·®: {np.std(correct_scores):.6f}")
        print(f"     æœ€å°å€¼: {np.min(correct_scores):.6f}")
        print(f"     æœ€å¤§å€¼: {np.max(correct_scores):.6f}")
    
    # æ„å¤–æƒ…å†µåˆ†æ
    print("\n3. æ„å¤–æƒ…å†µåˆ†æ:")
    if len(contribution_data['flipped_scores']) > 0:
        flipped_positive = np.sum(contribution_data['flipped_scores'] >= 0)
        correct_negative = np.sum(contribution_data['correct_scores'] < 0)
        
        print(f"   ç¿»è½¬æ ·æœ¬ä¸­è´¡çŒ®å€¼ >= 0 çš„æ•°é‡: {flipped_positive}")
        print(f"   ç¿»è½¬æ ·æœ¬ä¸­è´¡çŒ®å€¼ >= 0 çš„æ¯”ä¾‹: {flipped_positive / len(contribution_data['flipped_scores']) * 100:.2f}%")
        print(f"   æ­£ç¡®æ ·æœ¬ä¸­è´¡çŒ®å€¼ < 0 çš„æ•°é‡: {correct_negative}")
        print(f"   æ­£ç¡®æ ·æœ¬ä¸­è´¡çŒ®å€¼ < 0 çš„æ¯”ä¾‹: {correct_negative / len(contribution_data['correct_scores']) * 100:.2f}%")
        
        # ç†è®ºéªŒè¯
        theory_check = np.mean(contribution_data['flipped_scores']) < np.mean(contribution_data['correct_scores'])
        print(f"   ç†è®ºéªŒè¯(ç¿»è½¬æ ·æœ¬å¹³å‡å€¼ < æ­£ç¡®æ ·æœ¬å¹³å‡å€¼): {'âœ… é€šè¿‡' if theory_check else 'âŒ æœªé€šè¿‡'}")
    else:
        negative_count = np.sum(contribution_data['correct_scores'] < 0)
        print(f"   æ‰€æœ‰æ ·æœ¬ä¸­è´¡çŒ®å€¼ < 0 çš„æ•°é‡: {negative_count}")
        print(f"   æ‰€æœ‰æ ·æœ¬ä¸­è´¡çŒ®å€¼ < 0 çš„æ¯”ä¾‹: {negative_count / len(contribution_data['correct_scores']) * 100:.2f}%")
    
    # æœ€ä½³é˜ˆå€¼åˆ†æ
    if optimal_results and 'best_threshold' in optimal_results:
        print("\n4. æœ€ä½³é˜ˆå€¼åˆ†æ:")
        print(f"   æœ€ä½³é˜ˆå€¼: {optimal_results['best_threshold']:.6f}")
        print(f"   æœ€ä½³F1åˆ†æ•°: {optimal_results['best_f1']:.4f}")
        print(f"   å¯¹åº”ç²¾ç¡®ç‡: {optimal_results['metrics']['precision']:.4f}")
        print(f"   å¯¹åº”å¬å›ç‡: {optimal_results['metrics']['recall']:.4f}")
        
        # æ¯”è¾ƒä¸åŒé˜ˆå€¼
        print("\n5. é˜ˆå€¼æ¯”è¾ƒ:")
        for threshold in [0.0, optimal_results['best_threshold']]:
            eval_result = evaluate_threshold(contribution_data, threshold)
            if eval_result:
                print(f"   é˜ˆå€¼ {threshold:.6f}:")
                print(f"     æ£€æµ‹æ ·æœ¬æ•°: {eval_result['num_detected']}")
                print(f"     æ£€æµ‹ç‡: {eval_result['detection_rate']*100:.2f}%")
                print(f"     ç²¾ç¡®ç‡: {eval_result['precision']:.4f}")
                print(f"     å¬å›ç‡: {eval_result['recall']:.4f}")
                print(f"     F1åˆ†æ•°: {eval_result['f1_score']:.4f}")
                print(f"     TP: {eval_result['tp']}, FP: {eval_result['fp']}, FN: {eval_result['fn']}, TN: {eval_result['tn']}")
                print()
    
    # æ¨èå»ºè®®
    print("\n6. æ¨èå»ºè®®:")
    if len(contribution_data['flipped_scores']) > 0:
        if optimal_results and optimal_results['best_f1'] > 0.7:
            print("   âœ… æ£€æµ‹æ€§èƒ½è‰¯å¥½ï¼Œæ¨èä½¿ç”¨æœ€ä½³é˜ˆå€¼è¿›è¡Œæ£€æµ‹")
        elif optimal_results and optimal_results['best_f1'] > 0.5:
            print("   âš ï¸ æ£€æµ‹æ€§èƒ½ä¸­ç­‰ï¼Œå»ºè®®ç»“åˆå…¶ä»–æ–¹æ³•éªŒè¯")
        else:
            print("   âŒ æ£€æµ‹æ€§èƒ½è¾ƒå·®ï¼Œå»ºè®®é‡‡ç”¨å…¶ä»–æ–¹æ³•æˆ–è°ƒæ•´æ•°æ®")
        
        flipped_negative_ratio = np.sum(contribution_data['flipped_scores'] < 0) / len(contribution_data['flipped_scores'])
        if flipped_negative_ratio > 0.8:
            print("   ğŸ’¡ å¤§éƒ¨åˆ†ç¿»è½¬æ ·æœ¬è´¡çŒ®å€¼ä¸ºè´Ÿï¼Œç¬¦åˆç†è®ºé¢„æœŸ")
        else:
            print("   ğŸ¤” éƒ¨åˆ†ç¿»è½¬æ ·æœ¬è´¡çŒ®å€¼ä¸ºæ­£ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
    else:
        print("   â„¹ï¸ æ²¡æœ‰ç¿»è½¬æ ·æœ¬ï¼Œæ— æ³•è¿›è¡Œé”™è¯¯æ£€æµ‹åˆ†æ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹Shapleyå€¼è´¡çŒ®åˆ†æå’Œé˜ˆå€¼ä¼˜åŒ–")
    
    # åŠ è½½æ•°æ®
    try:
        shapley_data = load_shapley_data('resnet')
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("è¯·å…ˆè¿è¡Œç›¸å…³è„šæœ¬ç”ŸæˆShapleyå€¼æ•°æ®")
        return
    
    # åˆ†æè´¡çŒ®å€¼åˆ†å¸ƒ
    print("\nğŸ“ˆ åˆ†æè´¡çŒ®å€¼åˆ†å¸ƒ...")
    contribution_data = analyze_contribution_distribution(shapley_data)
    
    # å¯»æ‰¾æœ€ä½³é˜ˆå€¼
    print("\nğŸ¯ å¯»æ‰¾æœ€ä½³é˜ˆå€¼...")
    optimal_results = {}
    if len(contribution_data['flipped_scores']) > 0:
        best_threshold, best_f1, metrics = find_optimal_threshold(contribution_data)
        optimal_results = {
            'best_threshold': best_threshold,
            'best_f1': best_f1,
            'metrics': metrics
        }
        print(f"   æœ€ä½³é˜ˆå€¼: {best_threshold:.6f}")
        print(f"   æœ€ä½³F1åˆ†æ•°: {best_f1:.4f}")
    else:
        print("   æ²¡æœ‰ç¿»è½¬æ ·æœ¬ï¼Œæ— æ³•è®¡ç®—æœ€ä½³é˜ˆå€¼")
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡
    print_detailed_statistics(contribution_data, optimal_results)
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    save_path = os.path.join(os.getcwd(), 'threshold_analysis_results.png')
    plot_analysis_results(contribution_data, optimal_results, save_path)
    
    print("\nâœ… åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()